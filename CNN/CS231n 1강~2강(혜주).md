## CS231n 1강~2강(혜주)

다음은 스탠포드 대학 강의  [CS231n: Convolutional Neural Networks for Visual Recognition](http://cs231n.stanford.edu/) 을 요약한 것입니다.



1강~2강의 내용을 도식화해서 정리하면 다음과 같습니다. 


![default](https://user-images.githubusercontent.com/32008883/30751850-9303cba6-9ff5-11e7-9f0d-cb28d8087e58.JPG)



## 1.이미지 처리 insight(1강)

1) 고양이가 어떻게 이미지를 인식할까? 에 대한 연구 (Hubel & Wiesel,1959)

![cs231n_week1_1](https://user-images.githubusercontent.com/32008883/30751966-dea3b4c2-9ff5-11e7-870d-44140ff3b480.JPG)


고양이가 이미지를 처리하는 방법을 살펴보니, 각기 다른 뉴런이 이미지를 나누어 인식한다.  복잡한 이미지를 단순한 이미지로 나누어서 처리한다.

2) 복잡한 물체(object)를 간단한 것의 결합으로 보는 시선

![cs231n_week1_2](https://user-images.githubusercontent.com/32008883/30751987-f03d50e4-9ff5-11e7-8650-70da272845b6.JPG)

3) 특성 중심(feature based)의 이미지 처리 

![cs231n_week1_3](https://user-images.githubusercontent.com/32008883/30752062-1eba20aa-9ff6-11e7-92d5-38e5bb037353.JPG)

이미지에서 중요한 특성만 추출해서 그 특징을 바탕으로 이미지를 처리한다. 예를 들어 위의 표지판에서는 'STOP' 이라는 글자가 '특성(feature)'이 된다. 

4) 이미지넷 챌린지(IMAGENET Challenge)

![cs231n_week1_4](https://user-images.githubusercontent.com/32008883/30752067-21d78aac-9ff6-11e7-9f14-c279c2e63cf3.JPG)

 십만 개가 넘는 이미지를 1000개의 물체로 분류하는 챌린지다.
 ![cs231n_week1_5](https://user-images.githubusercontent.com/32008883/30752076-258dec72-9ff6-11e7-95bd-5ddb9a074c2b.JPG)

위의 그래프는 이미지 분류 챌린지의 오류 정도가 낮아짐을 알 수 있다. 주목할 만한 것은 2012 년도인데, 앞으로 강의에서 배울 'Convolutional Neural Network(CNN)'를 적용한 년도이기도 하다.
![cs231n_week1_6](https://user-images.githubusercontent.com/32008883/30752073-255fcedc-9ff6-11e7-94af-2fe6c78d2ce4.JPG)

하지만 CNN은 새로 등장한 개념이 아니다. 1998년도에도 손글씨를 인식하여 우편을 분류하려는 노력이 있었는데, 2012년도의 CNN과 매우 유사함을 알 수 있다.

4) 하지만 아직 갈 길은 멀다.. (to be continued..)
![cs231n_week1_7](https://user-images.githubusercontent.com/32008883/30752075-258195bc-9ff6-11e7-9f6c-4b57682c0c21.JPG)

위의 이미지를 기계가 파악할 수 있을까? '사람들은 몸무게에 예민하다','대통령이 이런 장난을 치는 것은 흔하지 않은 일이다' 등 다양한 맥락적 요소가 포함되어 있는 것이다. 즉, 사람의 시력은 단순한 것이 아니라 지능 (visual intelligence)도 포함되어 있다.  갈 길이 멀다는 뜻이다. 하지만 컴퓨터의 이미지 처리 관련 기술이 발전해야 삶의 질을 개선할 수 있다는 것 또한 팩트..





## 2.  이미지 분류의 어려움

저번 시간에 이어서 이미지 분류에는 여러 어려움을 소개한다.

우리는 의자를 옆에서 보든 앞에서 보든 거꾸로 보든 의자인지 알고 있다. 하지만 기계는 모른다. 이렇게 빛 노출에 따라 고양이인지 파악하기 어렵다.


![cs231n_week2_1](https://user-images.githubusercontent.com/32008883/30752113-32df721a-9ff6-11e7-9264-78ea0f54a2a2.JPG)






## 3. 해결책 :  Data-Driven Approach

그!래!서!  이미지 분류의 어려움을 해소하는데 도움을 주고자 데이터에 기반한 접근법(Data-Driven Approach)이 등장하였다.



#### Data-Driven Approach

#### 1) 이미지와 라벨(label) 데이터를 모은다. 

라벨이란 이미지를 분류하는 이름이라고 생각하면 된다. '고양이','비행기' 등이 예이다. 

#### 2) 머신러닝(Machinen Learning)을 통해 이미지 분류기(Classifier)를 학습시킨다.

#### 3) 새로운 이미지에 분류기를 적용하고 평가한다.









## 4. Classifier 1: Nearest Neighbor

주어진 이미지와 '가장 가까운 이웃' 이미지를 찾는 분류기이다.  이 분류기가 작동하는 원리는 다음과 같다.

### 1) 작동원리

1. 모든 데이터와 라벨을 학습시킨다.
2. 테스트용 이미지가 주어지면, 학습했던 이미지와 가장 유사한 라벨을 찾아준다. 



즉, 새로운 고양이 이미지가 주어지면, 학습했던 이미지 중 가장 유사한 라벨이 '고양이'라고 판단하는 분류기이다.



### 2) 그러면 어떤 이미지가 '가깝다'고 할 수 있을까? : L1 distance

![cs231n_week2_4jpg](https://user-images.githubusercontent.com/32008883/30752114-32e25ec6-9ff6-11e7-8841-c1bc7cd97d6e.JPG)

새롭게 들어온 고양이 이미지와 학습시켰던 고양이 이미지를 픽셀로 표현했을 때, 그 차이를 계산하여 더하는 방법이 'L1 distance'이다. 그 합이 이미지 간의 거리라고 할 수 있다. 



### 3) 단점: 빠른 학습, 느린 예측

Nearest Neighbor classifier은 모든 데이터를 단지 외우게 학습시키기 때문에 학습 속도는 빠르다. 하지만 예측할 때는 하나하나 대조하므로 시간이 너무 오래 걸린다. 

중요한 것은 '빠른 예측'인데, 뒤에서 배울 CNN은 학습 속도는 다소 느리지만 예측 속도는 빠르다.



### 4) K-Nearest Neighbors

#### 1. 개념

![cs231n_week2_6jpg](https://user-images.githubusercontent.com/32008883/30752116-32f54248-9ff6-11e7-89c8-bcc29031bb59.JPG)

단순히 근처 가까운 이웃의 라벨을 베끼기보다, K개의 가까운 점들에게서 투표를 받아서 해당 이미지가 무엇인지 결정하게 됩니다.

K가 커질수록 경계선 부분이 부드러워진 것을 알 수 있습니다.

하얀색 부분은 근처에 이웃이 아무도 없어서 하얀색으로 표시됩니다.

#### 2. L2 Distance

![cs231n_week2_7](https://user-images.githubusercontent.com/32008883/30752098-3259b102-9ff6-11e7-9c31-846ad80f671f.JPG)

K-Nearest Neighbor Classifier에서는 L2 distance라는 것이 등장합니다. L1은 좌표축이 달라지면, 거리가 달라질 수 있다. 하지만 L2는 원이기 때문에 좌표축이 바뀌어도 괜찮다.



#### 3. hyperparameter 정하기: 그러면 K에는 얼마를 넣어야 , 어떤 distance 기준을 선택해야 좋을까? 

상황에 따라 다르다는 당연한 얘기...

하지만 좀 더 좋은 hyperparameter를 고르기 위해서 방법이 있다. 

데이터를 [Train set], [Validation set], [Test set]으로 분류한다.

1) Train set: 학습시킬 데이터이다.

2) Validation set: 학습 후 validation set에 각기 다른 hyperparmeter를 적용시키면서 어떤 hyperparmeter가 가장 예측률이 좋은지 파악한다.

3) Test set: 모델이 한 번도 본 적 없는 새로운 데이터여야 하며, 위에서 정한 hyperparameter를 적용하여 최종 예측률을 계산한다.

![cs231n_week2_9_1](https://user-images.githubusercontent.com/32008883/30752100-3281d5ce-9ff6-11e7-9122-925aede621ba.jpg)



#### 4. 하지만 K-Nearest Neighbor는 이미지 처리에 쓰이지 않는다...문제점은

1. 아까 언급했듯이 예측 속도가 매우 느리다.

2. L1 distance, L2 distance는 이미지 간 차이를 완벽하게 설명하지 못한다.예를 들어 하단의 4가지 이미지는 모두 같은 L2 distance를 가진다.

![cs231n_week2_10_1](https://user-images.githubusercontent.com/32008883/30752107-32b40f44-9ff6-11e7-81cc-5994f19e2e2e.jpg)

   ​








## 5. Classifier 2: Linear Classifier 

#### 1. 개념

두번째 이미지 분류기는 Linear Classifier이다. 

아래 보여지는  f(x,W) = Wx + b 가 Linear Classifier의 기본 식이다. 수식이 나왔다고 무서워하지 말고 간단히 생각해보자. (사실 본인도 무서움)

동물들의 이미지가 input데이터(x)로 주어졌고, 이미지를 분류하는 라벨의 개수는 10개라고 하자. 먼저 위 수식을 하나하나씩 살펴보자.

1) x (3072x1): 이미지 데이터(32x32x3)를 펼쳐서 하나의 벡터로 만든다. 

2) f(x,W): 분류되는 라벨과 관련된 점수를 보여주는데, 높을 수록 그 라벨에 속할 확률이 높음을 뜻한다. 라벨의 수가 10이므로 10x1 벡터이다.

3) W(10x3072) : W와 x를 곱하여 f(x,W) 가 나오는 것이므로 크기는 10x3072이다. W는 학습이 시작될 때 랜덤으로 정해진다. 학습이 지속되면서 고양이 이미지를 넣으면 고양이 라벨에 해당할 확률을 나타내는 점수가 높아지게끔 W가 조정된다.   

4) b는 bias term이라고 하는데, 데이터의 독립성을 조정하는 항이다. 예를 들어 데이터에 개보다 고양이의 이미지가 더 많다면 고양이에 대한 bias를 더 높게 잡아 독립성을 보완한다.



![cs231n_week2_12](https://user-images.githubusercontent.com/32008883/30752104-32ab5d36-9ff6-11e7-83c1-3583a0f2a7c6.JPG)



![cs231n_week2_13](https://user-images.githubusercontent.com/32008883/30752106-32b0ccbc-9ff6-11e7-846c-3745de505460.JPG)


#### 2. W

아래 왼쪽에서 다섯번째 이미지를 보자. 사슴에 대한 W의 행을 뽑아서 이미지로 나타낸 것이다. 희여멀건하지만 풀이라고 추정되는 녹색 바탕과 갈색의 물체가 보인다. 학습 데이터의 이미지를 하나의 템플릿으로 표현하려고 하다보니 뿌옇다.

![cs231n_week2_14](https://user-images.githubusercontent.com/32008883/30752105-32ae15bc-9ff6-11e7-8ff7-6b84bb4d8ef0.JPG)



#### 3. 한계

![cs231n_week2_15](https://user-images.githubusercontent.com/32008883/30752108-32bb04f2-9ff6-11e7-8f9c-d86d5501ffb6.JPG)

linear classifier도 한계가 있는데, 첫번째 케이스처럼 나누어져있다면 하나의 선으로 구역을 나눌 수 없다. 두번째 도넛과 같은 경우도 마찬가지이다. 세번째 경우도 세가지 덩어리(?)가 흩어져 있어 하나의 선으로 표현하기 어렵다.




