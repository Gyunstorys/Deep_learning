# CS231n 6강(혜주)

다음은 스탠포드 대학 강의  [CS231n: Convolutional Neural Networks for Visual Recognition](http://cs231n.stanford.edu/) 을 요약한 것입니다.



6강의 내용을 도식화해서 정리하면 다음과 같습니다. 

![cs231n_6_](https://user-images.githubusercontent.com/32008883/32380789-642cb69c-c0f4-11e7-8fa3-51a4d03ab504.JPG)





## 1. Activation Functions

### 1) Sigmoid function

![cs231n_6_1](https://user-images.githubusercontent.com/32008883/32380799-694b7d70-c0f4-11e7-9a20-f24213ac281e.JPG)

- 특징 
  - output의 범위가 0과 1이 되게 한다.
  - neuron의 firing rate을 잘 나타낸다고 생각되어서 인기가 많았다.
- 단점
  - 극단값들의 gradient는 결국 0이 되어 의미가 없어진다.
  - sigmoid의 output은 0이 중심이 되지 않는다. -> 항상 양수인 input이 들어오면 w에 대한 gradient는 모두 양수이거나 음수가 된다 -> w vector가 비효율적으로 찾아진다.
  - 함수에 exp()이 들어가서 computing이 비효율적이다.



### 2) ReLU function

![cs231n_6_6](https://user-images.githubusercontent.com/32008883/32381234-894a37a0-c0f5-11e7-9129-6cd0320e03a3.JPG)

**f(x) = max(0,x)**



- 특징
  - (+)부분에서 output 부분을 뭉개지 않는다.
  - exp()이 없어서 컴퓨팅적으로 효율적이다.
  - sigmoid나 tanh보다 빨리 converge한다.
  - sigmoid보다 neuron의 작용을 더 잘 반영한다.
- 단점
  - 역시 0이 중심인 output은 아니다.
  - 0보다 작은 부분의 gradient는 0이 되어버린다.



ReLU를 보완한 Leaky ReLU, PReLU등이 있다.

실제적으로 ReLU를 사용하면 좋다. (단, learning rate설정 유의)

sigmoid는 별로 효과가 좋지 않다. 초반에 나온 것이라 사용되었던 거지 지금은 ReLU를 많이 쓴다고 한다.





## 2. Data Preprocessing

![cs231n_6_10](https://user-images.githubusercontent.com/32008883/32381233-890cea4e-c0f5-11e7-93b6-a1a2c9f00dd0.JPG)

데이터 전처리에 주로 zero-centering을 많이 사용한다. 이미지 처리에서는 normalization까지 하지는 않는다.



## 3. Weight Initialization

weight을 처음에 어떻게 시작하는지에 따라 학습 과정이 달라질 수 있으므로 weight initialization은 중요하다.

1) 작은 랜덤 숫자로 시작? **sd=0.01**

**W = 0.01* np.random.randn(D,H)** 

작은 네트워크에서는 괜찮지만, 네트워크의 깊이가 깊어지면 분산이 0으로 가기 때문에, 아래와 같이 데이터들이 줄어든다. 즉, 모든 activiation이 0 이 되어버린다. 이에 따라 upstreaming gradient가 0이 된다. 


![cs231n_6_12](https://user-images.githubusercontent.com/32008883/32381720-bd26cc5e-c0f6-11e7-99b2-f131d5da11ed.JPG)



2) **sd=1로 설정하면?**

하단과 같이 모든 뉴런이 -1아니면 1이 되어버려 gradient는 모두 0 이 되어버린다.




![cs231n_6_14](https://user-images.githubusercontent.com/32008883/32381722-bddd81d8-c0f6-11e7-8bf2-d26b8c0aff12.JPG)


3) Xavier Initialization

``` W = np.random.randn(fan_in,fan_out) / np.sqrt(fan_in)``` 

적절한 initialization을 만들어주는 방법이다.

![cs231n_6_16](https://user-images.githubusercontent.com/32008883/32381951-5cf2ea06-c0f7-11e7-820a-a6104ddae04a.JPG)

그러나 ReLU에 사용하면 제대로 작동하지 않는다. 하단과 같이 보충한다.

 ` W = np.random.randn(fan_in,fan_out) / np.sqrt(fan_in/2)`

 

## 4. Batch Normalization

batch normalization은 batch의 dimension마다  unit gaussain activation을 만들어주는 과정이다.



![cs231n_6_19](https://user-images.githubusercontent.com/32008883/32381949-5c497f2a-c0f7-11e7-9da8-59cf46e6615f.JPG)
![cs231n_6_20](https://user-images.githubusercontent.com/32008883/32381950-5ca33448-c0f7-11e7-8d36-eb489b7a3f02.JPG) 



![cs231n_6_21](https://user-images.githubusercontent.com/32008883/32383005-214accb4-c0fa-11e7-9926-76e5b2d2c91a.JPG)

1. dimension에 따라 평균과 분산 계산
2. 식에 따라 normalize

![cs231n_6_22](https://user-images.githubusercontent.com/32008883/32383006-217d1a70-c0fa-11e7-8b2a-ca25145dcb9d.JPG)

- 위치: FC layer 혹은 Convolutional layer 뒤, 그리고 nonlinearity function 앞.
- normalize 한 것을 다시 되돌리기 위해 하단의 식을 사용한다.

![cs231n_6_23](https://user-images.githubusercontent.com/32008883/32382999-2089f6a6-c0fa-11e7-8c9f-d4a95967bd34.JPG)
![cs231n_6_24](https://user-images.githubusercontent.com/32008883/32383002-20f0258e-c0fa-11e7-97a7-77e0c36f39a6.JPG)

- 특징
  - gradient flow 향상
  - 좀 더 높은 learning rate 가능하게 함
  - initialization에 대한 의존도를 줄인다



## 5. Babysitting Learning Process

학습 과정을 간단히 정리하면, 

첫번째 step: 데이터 전처리

두번쨰 step: 구조를 선택한다.



![cs231n_6_25](https://user-images.githubusercontent.com/32008883/32383003-211e9522-c0fa-11e7-98d5-40138c179a2a.JPG)

- loss가 적절한지 확인하는 법
  - regularization term을 없앴다가 만든다. regularization term을 넣어서 loss가 올라가면 잘 된 거다.
  - loss가 줄어들지 않고 그대로? : learning rate너무 낮아
  - loss가 폭발적?:  learning rate너무 높아
  - cross validating할 때 대충 [1e-3,..1e-5]면 적당하다고 한다.



![cs231n_6_26](https://user-images.githubusercontent.com/32008883/32383250-d9ed9d64-c0fa-11e7-8442-d76dac3b605e.JPG)
![cs231n_6_27](https://user-images.githubusercontent.com/32008883/32383251-da189b9a-c0fa-11e7-8a65-559090a0649e.JPG)



- Cross-validation 전략(coarse -> fine)
  - 먼저 coarse하게 대충 어떻게 parameter들이 작동하는지 파악
  - 이후 fine 하게 parameter찾기




![cs231n_6_32](https://user-images.githubusercontent.com/32008883/32383475-67c6a2c0-c0fb-11e7-80de-75631fcf102b.JPG)



![cs231n_6_33](https://user-images.githubusercontent.com/32008883/32383467-6655f346-c0fb-11e7-9e80-5a9b206e53b4.JPG)



- Random Search가 Grid Search보다 좋다.

![cs231n_6_34](https://user-images.githubusercontent.com/32008883/32383469-668ef088-c0fb-11e7-896b-1a6727578a51.JPG)



- learning rate말고도 network 구조, decay schedule, update type, regularization등 조정해야 할게 많다. 이에 따라 loss function의 작동이 달라지므로.





##6. Hyperparmeter Optimization

1) loss curve보기

good learning rate의 모양 잘 봐두자!

![cs231n_6_36](https://user-images.githubusercontent.com/32008883/32383637-de735684-c0fb-11e7-8d29-097b0695a987.JPG)

다음과 같은 형태의 loss curve하면 bad initialization을 의심해봐야 한다.

![cs231n_6_37](https://user-images.githubusercontent.com/32008883/32383640-dede0a92-c0fb-11e7-8803-38d430ca9165.JPG)

- train set accuracy vs. validation set accuracy
  - 큰 차이: overfitting문제
  - 차이 x : overfitting x

![cs231n_6_38](https://user-images.githubusercontent.com/32008883/32383632-ddb74ffc-c0fb-11e7-9f10-9f89e45011ce.JPG)

- weight의 update비율을 확인하고 조정하는 것도 좋은 방법!

![cs231n_6_39](https://user-images.githubusercontent.com/32008883/32383634-ddf1b336-c0fb-11e7-9597-286125e99976.JPG)



## + 강의자님의 요약..

![cs231n_6_40](https://user-images.githubusercontent.com/32008883/32383635-de2c10d0-c0fb-11e7-9a41-6f2c8b7c4ad9.JPG)

