{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# N-GRAM 을 이용한 word embedding 구현\n",
    "## YBIGTA _ 이상헌\n",
    "\n",
    "\n",
    "## pytorch 튜토리얼 겸용\n",
    "pytorch는 다음과 같은 프로세스로 진행된다 :\n",
    "1. 데이터 정의\n",
    "2. 모델 정의\n",
    "3. Loss function 정의 \n",
    "4. 역전파 계산\n",
    "5. 계산을 통한 최적화.\n",
    "\n",
    "이 프로세스를 ** N GRAM language modeling ** 으로 진행한다.\n",
    "\n",
    "\n",
    "## 0.1. N-gram lang modeling?\n",
    "\n",
    "## 0.2. What is word embedding?\n",
    "\n",
    "word embedding이란 자신의 단어사전에 각 단어가 해당하는 *벡터*를 지칭하는 말이다. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 데이터 정의\n",
    "\n",
    "본 테스트는 필자가 가장 좋아하는 다이나믹 듀오의 *죽일놈*으로 테스트한다. <br>\n",
    "보통 N-gram language model training 의 경우 100 million words가 넘어야 하므로 toy model이라고 받아들인다. <br>\n",
    "본 테스트는 pytorch tutorial과 같이 trigram으로 진행한다. <br>\n",
    "한국어의 경우는 영어와 달라 본 프로세스 전, 형태소 분석을 통해 조사를 제거해주는 작업이 필요하다. <br>\n",
    "하지만 본 튜토리얼에서 해당 과정은 스킵한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(['너', '아까'], '나한테'), (['아까', '나한테'], '왜'), (['나한테', '왜'], '그랬어')]\n"
     ]
    }
   ],
   "source": [
    "test_sentence = \"\"\"너 아까 나한테 왜 그랬어\n",
    "너 또 왜 그러는데\n",
    "내가 도대체 어디까지 맞춰야 돼\n",
    "넌 맨날 그런 식이야 됐어 나 갈게\n",
    "\n",
    "너는 뛰쳐나가 차 문을 부술 듯이 닫으면서\n",
    "난 머리를 처박고 한숨 쉬어 핸들을 안으면서\n",
    "이런 광경이 너무 익숙해 이젠\n",
    "웬만한 싸움에는 상처도 잘 안 나 이젠\n",
    "명품 쇼핑할 때처럼 너무 깐깐해 네 기준은\n",
    "한번 화내면 뒤끝 장난 아냐 적어도 2주는 가니까\n",
    "난 성격이 너무 물러서 \n",
    "넌 항상 말해 남자니까 뒤로 좀 물러서\n",
    "부담돼 네가 내게 결혼을 보채는 것도 \n",
    "난 달인처럼 대화 화제를 돌리는 법도 많이 늘었어\n",
    "넌 항상 추격하고 나는 도망쳐\n",
    "솔직히 말할게 난 아직 준비 안 됐어\n",
    "지쳤어 조금 널 향한 사랑은 도금이 \n",
    "벗겨진 반지처럼 빛이 바랬어 \n",
    "오늘은 이별을 말해야 될 것 같아 \n",
    "지겹거든 너랑 다툴 때마다 항상 하는 말\n",
    "내가 죽일 놈이지 뭐 우리가 어긋날 때면\n",
    "전부 내 탓이지 뭐 마치 죄인인 것처럼\n",
    "난 한걸음 물러서서 아무 말도 안 해\n",
    "완벽한 너한테 난 항상 부족한 사람인 걸\n",
    "처음에 쉬웠어 너와 함께라는 게 \n",
    "난 너를 위해 내 자신을 숨기고 또 지웠어\n",
    "사랑에 취해 네게 기대고\n",
    "너란 퍼즐에 날 억지로 맞춰 끼웠어\n",
    "하지만 이젠 나 지쳤어 \n",
    "네가 만든 내게 난 숨이 막혀오는데\n",
    "넌 점점 더 내게 바라는 게 많아졌어\n",
    "마찰이 잦아졌어 네가 사준 \n",
    "구두굽처럼 사랑이 닳아졌어\n",
    "네 잔소리는 넥타이처럼 \n",
    "내 목을 조여서 날 얌전하게 만들었지\n",
    "그래서 그게 좋아 보였어 \n",
    "그때 내 속은 한참 뒤틀리고 꼬였어\n",
    "지금 난 널 만나기 전에 내가 너무 고파\n",
    "이미 우리 사이 권태라는 벽은 너무 높아\n",
    "내가 더 잘 할게 잘 할게 하며 발악했던 나지만\n",
    "오늘은 말할래 이것이 너와 나의 마지막\n",
    "내가 죽일 놈이지 뭐 우리가 어긋날 때면\n",
    "전부 내 탓 이지 뭐 마치 죄인인 것처럼\n",
    "난 한걸음 물러서서 아무 말도 안 해\n",
    "완벽한 너한테 나 항상 부족한 사람인 걸\n",
    "내가 잘 할게\n",
    "내가 잘 할게란 말 이제 두 번 다시 안 할게\n",
    "이 말 안 할래\n",
    "너를 사랑해란 말 이제 두 번 다시 안 할게\n",
    "내가 잘 할게\n",
    "내게 잘해달란 말 이제 두 번 다시 안할게\n",
    "이 말 안 할래\n",
    "그동안 참아왔던 이별을 오늘은 네게 말할래\n",
    "\n",
    "내가 죽일 놈이지 뭐 우리가 어긋날 때면\n",
    "전부 내 탓 이지 뭐 마치 죄인인 것처럼\n",
    "난 한걸음 물러서서 아무 말도 안 해\n",
    "완벽한 너한테 나 항상 부족한 사람인 걸.\"\"\".split()\n",
    "\n",
    "CONTEXT_SIZE = 2 # trigram이기 때문에 context size = 2이다.\n",
    "# Trigram 변환\n",
    "trigrams = [([test_sentence[i], test_sentence[i + 1]], test_sentence[i + 2])\n",
    "            for i in range(len(test_sentence) - 2)]\n",
    "# print the first 3, just so you can see what they look like\n",
    "print(trigrams[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'2주는': 102,\n",
       " '가니까': 39,\n",
       " '갈게': 151,\n",
       " '같아': 179,\n",
       " '걸': 100,\n",
       " '걸.': 113,\n",
       " '것': 161,\n",
       " '것도': 79,\n",
       " '것처럼': 112,\n",
       " '게': 149,\n",
       " '결혼을': 158,\n",
       " '고파': 72,\n",
       " '광경이': 173,\n",
       " '구두굽처럼': 122,\n",
       " '권태라는': 33,\n",
       " '그게': 98,\n",
       " '그동안': 136,\n",
       " '그때': 66,\n",
       " '그래서': 97,\n",
       " '그랬어': 77,\n",
       " '그러는데': 36,\n",
       " '그런': 137,\n",
       " '기대고': 92,\n",
       " '기준은': 109,\n",
       " '깐깐해': 140,\n",
       " '꼬였어': 41,\n",
       " '끼웠어': 90,\n",
       " '나': 164,\n",
       " '나는': 49,\n",
       " '나의': 14,\n",
       " '나지만': 29,\n",
       " '나한테': 194,\n",
       " '난': 155,\n",
       " '날': 94,\n",
       " '남자니까': 63,\n",
       " '내': 86,\n",
       " '내가': 85,\n",
       " '내게': 152,\n",
       " '너': 132,\n",
       " '너는': 82,\n",
       " '너란': 23,\n",
       " '너랑': 48,\n",
       " '너를': 103,\n",
       " '너무': 71,\n",
       " '너와': 24,\n",
       " '너한테': 80,\n",
       " '넌': 181,\n",
       " '널': 58,\n",
       " '네': 187,\n",
       " '네가': 130,\n",
       " '네게': 2,\n",
       " '넥타이처럼': 26,\n",
       " '놈이지': 7,\n",
       " '높아': 193,\n",
       " '늘었어': 117,\n",
       " '다시': 68,\n",
       " '다툴': 191,\n",
       " '닫으면서': 11,\n",
       " '달인처럼': 115,\n",
       " '닳아졌어': 52,\n",
       " '대화': 111,\n",
       " '더': 125,\n",
       " '도금이': 169,\n",
       " '도대체': 176,\n",
       " '도망쳐': 202,\n",
       " '돌리는': 127,\n",
       " '돼': 162,\n",
       " '됐어': 88,\n",
       " '될': 178,\n",
       " '두': 104,\n",
       " '뒤끝': 96,\n",
       " '뒤로': 195,\n",
       " '뒤틀리고': 143,\n",
       " '듯이': 89,\n",
       " '때마다': 180,\n",
       " '때면': 131,\n",
       " '때처럼': 108,\n",
       " '또': 73,\n",
       " '뛰쳐나가': 25,\n",
       " '마지막': 101,\n",
       " '마찰이': 154,\n",
       " '마치': 206,\n",
       " '막혀오는데': 44,\n",
       " '만나기': 56,\n",
       " '만든': 17,\n",
       " '만들었지': 50,\n",
       " '많아졌어': 38,\n",
       " '많이': 175,\n",
       " '말': 53,\n",
       " '말도': 37,\n",
       " '말할게': 184,\n",
       " '말할래': 107,\n",
       " '말해': 200,\n",
       " '말해야': 208,\n",
       " '맞춰': 105,\n",
       " '맞춰야': 156,\n",
       " '맨날': 27,\n",
       " '머리를': 69,\n",
       " '명품': 59,\n",
       " '목을': 138,\n",
       " '문을': 9,\n",
       " '물러서': 13,\n",
       " '물러서서': 54,\n",
       " '뭐': 40,\n",
       " '바라는': 34,\n",
       " '바랬어': 190,\n",
       " '반지처럼': 146,\n",
       " '발악했던': 171,\n",
       " '번': 22,\n",
       " '법도': 188,\n",
       " '벗겨진': 12,\n",
       " '벽은': 6,\n",
       " '보였어': 174,\n",
       " '보채는': 74,\n",
       " '부담돼': 157,\n",
       " '부술': 186,\n",
       " '부족한': 183,\n",
       " '빛이': 110,\n",
       " '사람인': 204,\n",
       " '사랑에': 20,\n",
       " '사랑은': 165,\n",
       " '사랑이': 10,\n",
       " '사랑해란': 31,\n",
       " '사이': 21,\n",
       " '사준': 189,\n",
       " '상처도': 129,\n",
       " '성격이': 185,\n",
       " '속은': 67,\n",
       " '솔직히': 35,\n",
       " '쇼핑할': 83,\n",
       " '숨기고': 106,\n",
       " '숨이': 126,\n",
       " '쉬어': 61,\n",
       " '쉬웠어': 87,\n",
       " '식이야': 65,\n",
       " '싸움에는': 18,\n",
       " '아까': 192,\n",
       " '아냐': 118,\n",
       " '아무': 8,\n",
       " '아직': 209,\n",
       " '안': 4,\n",
       " '안으면서': 51,\n",
       " '안할게': 47,\n",
       " '얌전하게': 135,\n",
       " '어긋날': 134,\n",
       " '어디까지': 78,\n",
       " '억지로': 119,\n",
       " '오늘은': 116,\n",
       " '완벽한': 5,\n",
       " '왜': 95,\n",
       " '우리': 150,\n",
       " '우리가': 0,\n",
       " '웬만한': 123,\n",
       " '위해': 172,\n",
       " '이': 124,\n",
       " '이것이': 182,\n",
       " '이런': 201,\n",
       " '이미': 16,\n",
       " '이별을': 43,\n",
       " '이제': 45,\n",
       " '이젠': 141,\n",
       " '이지': 99,\n",
       " '익숙해': 28,\n",
       " '자신을': 75,\n",
       " '잔소리는': 142,\n",
       " '잘': 147,\n",
       " '잘해달란': 93,\n",
       " '장난': 121,\n",
       " '잦아졌어': 114,\n",
       " '적어도': 160,\n",
       " '전부': 170,\n",
       " '전에': 148,\n",
       " '점점': 196,\n",
       " '조금': 30,\n",
       " '조여서': 76,\n",
       " '좀': 199,\n",
       " '좋아': 207,\n",
       " '죄인인': 15,\n",
       " '죽일': 153,\n",
       " '준비': 168,\n",
       " '지겹거든': 203,\n",
       " '지금': 205,\n",
       " '지웠어': 1,\n",
       " '지쳤어': 42,\n",
       " '차': 3,\n",
       " '참아왔던': 91,\n",
       " '처박고': 197,\n",
       " '처음에': 81,\n",
       " '추격하고': 62,\n",
       " '취해': 144,\n",
       " '탓': 64,\n",
       " '탓이지': 120,\n",
       " '퍼즐에': 19,\n",
       " '하는': 166,\n",
       " '하며': 128,\n",
       " '하지만': 70,\n",
       " '한걸음': 32,\n",
       " '한번': 84,\n",
       " '한숨': 159,\n",
       " '한참': 163,\n",
       " '할게': 60,\n",
       " '할게란': 57,\n",
       " '할래': 167,\n",
       " '함께라는': 145,\n",
       " '항상': 198,\n",
       " '해': 55,\n",
       " '핸들을': 139,\n",
       " '향한': 133,\n",
       " '화내면': 46,\n",
       " '화제를': 177}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 각 단어를 index화하는 작업이 필요하다. \n",
    "vocab = set(test_sentence)\n",
    "word_to_ix = {word: i for i, word in enumerate(vocab)}\n",
    "\n",
    "word_to_ix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "modeling을 위한 import\n",
    "\"\"\"\n",
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "word embedding의 dimension을 말하는 것.\n",
    "보통 정하는 기준이 무엇이지?\n",
    "이 hyperparam을 정하는 기준은 다음에 다룬다.\n",
    "ref ) https://arxiv.org/pdf/1601.00893.pdf\n",
    "\"\"\"\n",
    "EMBEDDING_DIM = 10\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 모델 정의\n",
    "\n",
    "모델을 정의한다. 모델을 정의하는 class는 nn.Module클래스를 input으로 받는데, <br>\n",
    "그 nn.Module이란 layer와 해당 layer에서 forwarding하는 함수를 가진다. <br>\n",
    "따라서 초기화 ` __init__ ` 에서 layer를 쌓고, <br>\n",
    "forwarding하는 함수를 쌓는다. <br>\n",
    "forwarding 하는 함수에는 초기화한 layer에 relu, batch등의 함수를 쌓아 만들고 <br>\n",
    "loss function 까지 쌓는다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "모델링.\n",
    "모델에 대한 그림은 칠판에 그리고 설명한다.\n",
    "1 layer 의 사이즈가 128인 이유는?\n",
    "\"\"\"\n",
    "\n",
    "class NGramLanguageModeler(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, embedding_dim, context_size):\n",
    "        super(NGramLanguageModeler, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.linear1 = nn.Linear(context_size * embedding_dim, 128)\n",
    "        self.linear2 = nn.Linear(128, vocab_size)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        embeds = self.embeddings(inputs).view((1, -1))\n",
    "        out = F.relu(self.linear1(embeds))\n",
    "        out = self.linear2(out)\n",
    "        log_probs = F.log_softmax(out)\n",
    "        return log_probs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. loss function 정의\n",
    "\n",
    "본 모델에서 loss function은 negative lognormal likelihood, 가장 단순한 모델을 사용한다. <br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "losses = [] # 각 epoch, 각 training step 마다 loss의 줄어듬을 보기위한 리스트\n",
    "loss_function = nn.NLLLoss() # Negative log likelihood loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 역전파 정의\n",
    "\n",
    "역전파의 함수는 input으로 model을 받는다. <br>\n",
    "따라서 모델을 먼저 초기화 하고, 그 다음 loss function안에 본 모델을 넣는다. <br>\n",
    "learning rate와 함께 선언하고, 본 예에서는 SGD를 사용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = NGramLanguageModeler(len(vocab), EMBEDDING_DIM, CONTEXT_SIZE) \n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001) # learning rate 는 보통 사용하는 0.001을 사용."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 최적화.\n",
    "\n",
    "최적화는 다음과 같은 과정을 거친다. <br>\n",
    "trigram model의 경우 index로 트레이닝을 한다. <br>\n",
    "(본 index 트레이닝이 실질적으로 어떠한 효과가 있는지는 QnA에서 다룬다.  <br>\n",
    "결국 input은 앞선 두 word의 index를 넣고, output은 training word의 index와 비교한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      " 55095.2969\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 55095.2969\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 55095.2969\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 55095.2969\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 55095.2969\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 55095.2969\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 55095.2969\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 55095.2969\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 55095.2969\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 55095.2969\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 55095.2969\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 55095.2969\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 55095.2969\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 55095.2969\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 55095.2969\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 55095.2969\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 55095.2969\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 55095.2969\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 55095.2969\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 55095.2969\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 55095.2969\n",
      "[torch.FloatTensor of size 1]\n",
      "]\n",
      "[\n",
      " 56721.0898\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 56721.0898\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 56721.0898\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 56721.0898\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 56721.0898\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 56721.0898\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 56721.0898\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 56721.0898\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 56721.0898\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 56721.0898\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 56721.0898\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 56721.0898\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 56721.0898\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 56721.0898\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 56721.0898\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 56721.0898\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 56721.0898\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 56721.0898\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 56721.0898\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 56721.0898\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 56721.0898\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 56721.0898\n",
      "[torch.FloatTensor of size 1]\n",
      "]\n",
      "[\n",
      " 58334.4570\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 58334.4570\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 58334.4570\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 58334.4570\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 58334.4570\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 58334.4570\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 58334.4570\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 58334.4570\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 58334.4570\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 58334.4570\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 58334.4570\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 58334.4570\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 58334.4570\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 58334.4570\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 58334.4570\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 58334.4570\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 58334.4570\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 58334.4570\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 58334.4570\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 58334.4570\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 58334.4570\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 58334.4570\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 58334.4570\n",
      "[torch.FloatTensor of size 1]\n",
      "]\n",
      "[\n",
      " 59935.3867\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 59935.3867\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 59935.3867\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 59935.3867\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 59935.3867\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 59935.3867\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 59935.3867\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 59935.3867\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 59935.3867\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 59935.3867\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 59935.3867\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 59935.3867\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 59935.3867\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 59935.3867\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 59935.3867\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 59935.3867\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 59935.3867\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 59935.3867\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 59935.3867\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 59935.3867\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 59935.3867\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 59935.3867\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 59935.3867\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 59935.3867\n",
      "[torch.FloatTensor of size 1]\n",
      "]\n",
      "[\n",
      " 61523.7891\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 61523.7891\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 61523.7891\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 61523.7891\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 61523.7891\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 61523.7891\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 61523.7891\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 61523.7891\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 61523.7891\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 61523.7891\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 61523.7891\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 61523.7891\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 61523.7891\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 61523.7891\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 61523.7891\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 61523.7891\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 61523.7891\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 61523.7891\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 61523.7891\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 61523.7891\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 61523.7891\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 61523.7891\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 61523.7891\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 61523.7891\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 61523.7891\n",
      "[torch.FloatTensor of size 1]\n",
      "]\n",
      "[\n",
      " 63099.5820\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 63099.5820\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 63099.5820\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 63099.5820\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 63099.5820\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 63099.5820\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 63099.5820\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 63099.5820\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 63099.5820\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 63099.5820\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 63099.5820\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 63099.5820\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 63099.5820\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 63099.5820\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 63099.5820\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 63099.5820\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 63099.5820\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 63099.5820\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 63099.5820\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 63099.5820\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 63099.5820\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 63099.5820\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 63099.5820\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 63099.5820\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 63099.5820\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 63099.5820\n",
      "[torch.FloatTensor of size 1]\n",
      "]\n",
      "[\n",
      " 64662.7305\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 64662.7305\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 64662.7305\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 64662.7305\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 64662.7305\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 64662.7305\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 64662.7305\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 64662.7305\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 64662.7305\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 64662.7305\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 64662.7305\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 64662.7305\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 64662.7305\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 64662.7305\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 64662.7305\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 64662.7305\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 64662.7305\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 64662.7305\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 64662.7305\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 64662.7305\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 64662.7305\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 64662.7305\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 64662.7305\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 64662.7305\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 64662.7305\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 64662.7305\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 64662.7305\n",
      "[torch.FloatTensor of size 1]\n",
      "]\n",
      "[\n",
      " 66213.1172\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 66213.1172\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 66213.1172\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 66213.1172\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 66213.1172\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 66213.1172\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 66213.1172\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 66213.1172\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 66213.1172\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 66213.1172\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 66213.1172\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 66213.1172\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 66213.1172\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 66213.1172\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 66213.1172\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 66213.1172\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 66213.1172\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 66213.1172\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 66213.1172\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 66213.1172\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 66213.1172\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 66213.1172\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 66213.1172\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 66213.1172\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 66213.1172\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 66213.1172\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 66213.1172\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 66213.1172\n",
      "[torch.FloatTensor of size 1]\n",
      "]\n",
      "[\n",
      " 67750.7891\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 67750.7891\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 67750.7891\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 67750.7891\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 67750.7891\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 67750.7891\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 67750.7891\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 67750.7891\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 67750.7891\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 67750.7891\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 67750.7891\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 67750.7891\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 67750.7891\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 67750.7891\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 67750.7891\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 67750.7891\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 67750.7891\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 67750.7891\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 67750.7891\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 67750.7891\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 67750.7891\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 67750.7891\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 67750.7891\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 67750.7891\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 67750.7891\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 67750.7891\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 67750.7891\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 67750.7891\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 67750.7891\n",
      "[torch.FloatTensor of size 1]\n",
      "]\n",
      "[\n",
      " 69275.5859\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 69275.5859\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 69275.5859\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 69275.5859\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 69275.5859\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 69275.5859\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 69275.5859\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 69275.5859\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 69275.5859\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 69275.5859\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 69275.5859\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 69275.5859\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 69275.5859\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 69275.5859\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 69275.5859\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 69275.5859\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 69275.5859\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 69275.5859\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 69275.5859\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 69275.5859\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 69275.5859\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 69275.5859\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 69275.5859\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 69275.5859\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 69275.5859\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 69275.5859\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 69275.5859\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 69275.5859\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 69275.5859\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 69275.5859\n",
      "[torch.FloatTensor of size 1]\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "# 총 10번 트레이닝\n",
    "for epoch in range(10):\n",
    "#    total_loss = torch.Tensor([0])\n",
    "    for context, target in trigrams:\n",
    "\n",
    "        # Step 1. Prepare the inputs to be passed to the model (i.e, turn the words\n",
    "        # into integer indices and wrap them in variables)\n",
    "        context_idxs = [word_to_ix[w] for w in context]\n",
    "        context_var = autograd.Variable(torch.LongTensor(context_idxs))\n",
    "\n",
    "        # Step 2. Recall that torch *accumulates* gradients. Before passing in a\n",
    "        # new instance, you need to zero out the gradients from the old\n",
    "        # instance\n",
    "        model.zero_grad()\n",
    "\n",
    "        # Step 3. Run the forward pass, getting log probabilities over next\n",
    "        # words\n",
    "        log_probs = model(context_var)\n",
    "\n",
    "        # Step 4. Compute your loss function. (Again, Torch wants the target\n",
    "        # word wrapped in a variable)\n",
    "        loss = loss_function(log_probs, autograd.Variable(\n",
    "            torch.LongTensor([word_to_ix[target]])))\n",
    "\n",
    "        # Step 5. Do the backward pass and update the gradient\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.data\n",
    "    losses.append(total_loss)\n",
    "    print(losses)  # The loss decreased every iteration over the training data!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:torch]",
   "language": "python",
   "name": "conda-env-torch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
