{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "작성자 : YBigta 10기 박승리"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8장 경사 하강법"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "대부분의 머신 러닝 기법은 주어진 상황에서 가장 적합한 모델을 찾아가는 과정으로 이루어져 있다.\n",
    "\n",
    "가장 적합한 모델이란\n",
    "- 모델의 오류(error)를 최소화 하는\n",
    "- 또는 모델의 우도(likelihood)를 최대화하는\n",
    "\n",
    "것을 의미한다.\n",
    " \n",
    "우리가 여러 모델을 최적화 시킬 때, 문제를 해결하기 위하여 **경사 하강법(gradient descent)** 이라는 방법을 사용할 것이다.\n",
    "\n",
    "이번장에서 우리는 **경사 하강법(gradient descent)**을 밑바닥에서부터 구현해 볼 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient란?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient(경사, 기울기)는 벡터 미적분학에서 scaler field의 최대 증가율을 나타내는 vector field를 의미한다.\n",
    "\n",
    "쉽게 말해, 함수가 **가장 빠르게 증가**할 수 있는 방향을 의미한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![rnn link](https://i.imgur.com/PxGH7qY.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "따라서 함수의 최댓값을 구하는 방법으로 gradient를 활용할 수 있다. \n",
    "\n",
    "임의의 시작점을 잡은 후 gradient를 계산하고, gradient의 방향으로 조금씩 이동하는 과정을 여러 번 반복하는 것이다.\n",
    "\n",
    "마찬가지로 함수의 최솟값은 gradient의 반대 방향으로 이동함으로써 구할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![rnn link](https://i.imgur.com/oj966PR.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math, random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient 계산하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "f가 변수가 하나인 함수인 경우, 점 x에서의 미분값은 미분의 정의에 따라 x가 아주 조금 변했을 때 f(x)의 변화량을 의미한다.\n",
    "\n",
    "x의 변화량을 h로 표기하며, h를 0에 근사시켜 아주 조금 변한다는 것을 표현한다.\n",
    "\n",
    "이 때의 미분값은 함수 변화율(difference quotient)의 극한값이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![rnn link](https://i.imgur.com/pZJZukj.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def difference_quotient(f, x, h):\n",
    "    return((f(x + h) - f(x)) / h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "미분값을 구하기 위해서는 도함수(derivative)를 구해야 한다.\n",
    "\n",
    "python으로는 극한값을 구할 수 없지만, 아주 작은 e값을 대입하여 근사시킨 값을 구할 수 있다.\n",
    "\n",
    "f(x) = x^2 함수의 도함수를 구해보겠다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_esimated_derivative():\n",
    "    \n",
    "    def square(x):\n",
    "        return x * x\n",
    "    \n",
    "    def derivative(x):\n",
    "        return 2 * x\n",
    "    \n",
    "    derivative_estimate = lambda x : difference_quotient(square, x, h = 0.00000000001)\n",
    "    \n",
    "    # 두 계산식에 따른 결과값이 거의 비슷함을 보여주기 위한 그래프\n",
    "    x = range(-10,10)\n",
    "    \n",
    "    plt.title(\"Actual Derivatives vs Estimates\")\n",
    "    plt.plot(x, list(map(derivative, x)), 'rx', label = 'Actual') # 빨간색 x\n",
    "    plt.plot(x, list(map(derivative_estimate, x)), '+b', label = 'Estimate') # 파란색 +\n",
    "    plt.legend(loc = 9)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAEICAYAAABcVE8dAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x23b19110b70>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_esimated_derivative()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "만약 f가 다변수 함수라면, 각각의 변수에 대해 작은 변화가 있을 때 f의 변화량을 알려주는 편도함수(partial derivative)가 여러 개 존재한다.\n",
    "\n",
    "i번째 편도함수는, i번째 변수를 제외한 다른 모든 입력변수를 고정시켜서 계산할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def partial_diffrence_quotient(f, v, i, h):\n",
    "    \n",
    "    # 함수 f의 i번째 편도함수가 v에서 가지는 값\n",
    "    w = [v_j + (h if j == i else 0)\n",
    "            for j, v_j in enumerate(v)]\n",
    "    \n",
    "    return (f(w) - f(v)) / h\n",
    "\n",
    "def estimate_gradient(f, v, h=0.00001):\n",
    "    return [partial_difference_quotient(f, v, i, h)\n",
    "            for i, _ in enumerate(v)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "하지만, 우리가 정의한 함수처럼 함수 두 값의 차이를 사용하여 근사하는 것은 계산해야 할 양이 많아진다는 단점이 있다.\n",
    "\n",
    "만약 v의 길이가 n이면(변수의 개수) estimate_gradient 함수는 총 2n번의 계산을 수행해야 하므로, 불필요한 계산이 많아진다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient 적용하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "함수 sum_of_square"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "함수 sum of square는 v가 0 벡터일 때 가장 작은 값을 가진다. \n",
    "\n",
    "우리는 경사 하강법을 이용하여 sum of square 3차원 벡터의 최솟값을 구하려고 한다.\n",
    "\n",
    "임의의 시작점을 잡고, gradient가 아주 작아질 때까지 경사의 반대 방향으로 조금씩 이동하면 구할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from scipy.spatial import distance\n",
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sum_of_squares(v):\n",
    "    return sum(v_i ** 2 for v_i in v)\n",
    "\n",
    "def step(v, direction, step_size):\n",
    "    # v에서 step_size만큼 이동하기\n",
    "    ls = []\n",
    "    for v_i, direction_i in zip(v, direction): # zip함수는 동일한 개수로 이루어진 자료형을 묶어주는 역할을 한다.\n",
    "        ls.append(v_i + step_size * direction_i)\n",
    "    return(np.array(ls))\n",
    "\n",
    "# sum_of_squares 함수의 도함수를 구해준다.\n",
    "def sum_of_squares_gradient(v):\n",
    "    return np.array([2 * v_i for v_i in v])\n",
    "\n",
    "def dist(x,y):   \n",
    "    return np.sqrt(np.sum((x-y)**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "minimum v [-0.00422561  0.00084512 -0.00253537]\n",
      "minimum value 2.49980817865e-05\n"
     ]
    }
   ],
   "source": [
    "# 임의의 시작점을 선택한다\n",
    "v = np.array([random.randint(-10,10) for i in range(3)])\n",
    "tolerance = 0.000001 # 수렴값\n",
    "\n",
    "while True:\n",
    "    gradient = sum_of_squares_gradient(v)   # v의 gradient 계산\n",
    "    next_v = step(v, gradient, -0.0001)     # gradient의 음수만큼 이동\n",
    "    if dist(next_v, v) < tolerance:     # tolerance에 수렴하게 되면 멈춘다\n",
    "        break\n",
    "    v = next_v                              # 수렴할 때까지 반복\n",
    "\n",
    "print(\"minimum v\", v)\n",
    "print(\"minimum value\", sum_of_squares(v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 적절한 step size 정하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "gradient를 따라 이동할 때, 어떤 step size가 최적의 결과를 낼 수 있는지 확인해야 한다.\n",
    "\n",
    "방법은 다음과 같다.\n",
    "- step size를 고정\n",
    "- 시간에 따라 step size를 점차 줄임\n",
    "- step을 거칠 때마다 목적 함수를 최소화하는 step size로 정함\n",
    "\n",
    "step size에 여러가지 값을 할당해서, 그 중에서 어떤 step size가 가장 효과적인지 찾아보려고 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "step_sizes = [100, 10, 1, 0.1, 0.01, 0.001, 0.0001, 0.00001]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그런데 어떤 x는 함수에 부적합한 값을 넣어서 오류를 발생시킬 수 있다. \n",
    "\n",
    "따라서 safe라는 함수를 통해, f에 오류가 발생했을 때 무한대(infinity)를 반환하는 함수를 정의한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def safe(f):\n",
    "    # f와 같은 함수를 반환하지만, f에 오류가 발생하면 무한대를 반환한다.\n",
    "    def safe_f(*args, **kwargs):\n",
    "        try:\n",
    "            return f(*args, **kwargs)\n",
    "        except:\n",
    "            return float('inf') # 무한대를 표기\n",
    "    return safe_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def minimize_batch(target_fn, gradient_fn, theta_0, tolerance=0.000001):\n",
    "    # 목적 함수(target_fn)을 최소화시키는 theta를 gradient descending을 사용해서 찾아준다.\n",
    "\n",
    "    step_sizes = np.array([100, 10, 1, 0.1, 0.01, 0.001, 0.0001, 0.00001])\n",
    "\n",
    "    theta = theta_0 # theta를 시작점으로 설정한다.\n",
    "    target_fn = safe(target_fn)\n",
    "    value = target_fn(theta) # 최소화시키려는 값\n",
    "\n",
    "    while True:\n",
    "        gradient = gradient_fn(theta)\n",
    "        next_thetas = [step(theta, gradient, -step_size) for step_size in step_sizes]\n",
    "\n",
    "        # 함수를 최소화시키는 theta를 선택한다.\n",
    "        # next_thetas에는 새롭게 gradient를 따라 이동한 x의 값들이 들어있다.\n",
    "        # 다음 theta, 즉 next_theta에는 target_fn(next_steps)의 값 중 가장 작은(목적함수) 값을 선택한다.\n",
    "        next_theta = min(next_thetas, key=target_fn)\n",
    "        next_value = target_fn(next_theta)\n",
    "\n",
    "        # 전 단계의 value와 이번 단계에서 새롭게 계산된 next_value의 값을 비교해, 그 차이가 tolerance만큼 수렴하면 멈춘다.\n",
    "        if abs(value - next_value) < tolerance:\n",
    "            return theta\n",
    "        else:\n",
    "            theta, value = next_theta, next_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-6  6  1]\n",
      "minimum v [-0.00099692  0.00099692  0.00016615]\n",
      "minimum value 2.01530993326e-06\n"
     ]
    }
   ],
   "source": [
    "v = np.array([random.randint(-10, 10) for i in range(3)])\n",
    "print(v)\n",
    "v = minimize_batch(sum_of_squares, sum_of_squares_gradient, v)\n",
    "print(\"minimum v\", v)\n",
    "print(\"minimum value\", sum_of_squares(v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic Gradient Descent (SGD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "앞에서 사용한 minimize_batch 함수는 이용하면 반복문을 한 번 돌 때마다 데이터 전체(모든 데이터 포인트, 모든 차원, 모든 변수)에 대한 gradient를 계산한다.\n",
    "\n",
    "위에서 실습한 예제는 데이터 포인트의 개수가 3개로 시간이 많이 걸리지 않았지만, 차원이 증가하면 계산 시간이 아주 오래 걸릴것이다.\n",
    "\n",
    "데이터 전체에 대한 오류값은 각각의 데이터 포인트에 대한 오류값들의 합과 같다.\n",
    "\n",
    "따라서 반복문을 한 번 돌 때마다 데이터 포인트 하나에 대한 gradient를 계산하는 SGD를 사용할 수 있다.\n",
    "\n",
    "한 번 반복문을 돌 때마다 임의의 순서대로 변수를 사용한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![rnn link](https://i.imgur.com/ZANr07V.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def in_random_order(data):\n",
    "    # 임의의 순서대로 data를 반환한다.\n",
    "    indexes = [i for i, _ in enumerate(data)] # data의 인덱스를 list로 생성\n",
    "    random.shuffle(indexes) # index를 섞는다.\n",
    "    for i in indexes:\n",
    "        yield data[i] # 섞인 순서대로 index를 반환"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SGD를 계산하면서, 함수값이 한동안 줄지 않으면 알고리즘을 종료하도록 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def minimize_stochastic(target_fn, gradient_fn, x, y, theta_0, alpha = 0.01):\n",
    "    # 데이터가 x, y 2개인 경우\n",
    "    \n",
    "    data = zip(x,y) # x = (3, 4, 5), y = (1, 5, 2)이면, zip(x,y) = ((3,1), (4,5), (5,2)) \n",
    "    theta = theta_0 # 처음 시작점\n",
    "    alpha = alpha_0 # 기본 step_size\n",
    "    min_theta, min_value = None, float('inf') # 시작할 때의 최솟값\n",
    "    iterations_with_no_implovement = 0\n",
    "    \n",
    "    while iterations_with_no_implovement < 100:\n",
    "        value = sum(target_fn(x_i, y_i, theta) for x_i, y_i in data)\n",
    "        \n",
    "        if value < min_value:\n",
    "            # 새로운 최솟값을 찾았다면, 그 값을 저장하고\n",
    "            # 처음 step size로 다시 돌아간다.\n",
    "            min_theta, min_value = theta, value\n",
    "            iterations_with_no_implovement = 0\n",
    "            alpha = alpha_0\n",
    "        else:\n",
    "            # 최솟값이 줄어들지 않는다면, step size를 축소한다.\n",
    "            iterations_with_no_implovement += 1\n",
    "            alpha *= 0.9\n",
    "            \n",
    "        # 각 변수에 대해 경사도를 계산한다. \n",
    "        # 반복문을 사용하여, 각 데이터 포인트에 대한 gradient를 계산한다.(위의 Minibatch Gradient Descent와의 차이 비교)\n",
    "        for x_i, y_i in in_random_order(data):\n",
    "            gradient_i = gradient_fn(x_i, y_i, theta)\n",
    "            theta = vector_subtract(theta, scalar_multiply(alpha, gradient_i))\n",
    "    \n",
    "    return min_theta"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
