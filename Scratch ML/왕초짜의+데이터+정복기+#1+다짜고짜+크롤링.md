
# 왕초짜의 데이터 정복기 

본인은 데이터 싸이언스에 입문한 왕 초짜이다.

통계는 평균이 무엇인지 정도 알고 프로그래밍은 파이썬으로 Hello! world 출력이 가능한 수준이다.

전공은 경제학과며 취업을 목전에 둔 4학년 학생이다.

본업인 경제학 공부를 열심히해서 금융공기업 준비를 하거나 고시를 하는게 올바르다하는 길이지만 한국사 시험만 따고 진작에 그만두었다.

하지만 왠지 인공지능에 매료되었고 머신러닝에 반했다.

그래서 데이터 싸이언스를 시작하고자 한다.

일기식으로 내가 배운것을 정리하고 기록하는 방식으로 글을 이어나가고자 한다.

만약 이 글은 읽는 이가 내 자신이 아닌 타인이라면 그대에게도 행운을 빌어주고 싶다.

거창한 제목과는 다르게 데이터 사이언스를 정복하긴 힘들 듯 싶다.

근데 뭐 혹시 모르니까

# #1 다짜고짜 크롤링

![image.png](attachment:image.png)

위의 책이 내가 선택한 첫번째 교재이다.

하지만 취업 스펙을 위해 일단 빅데이터 관련 공모전을 시작해버렸다.

![image.png](attachment:image.png)

그래서 이 책의 

# 9장인 크롤링

을 먼저하기로 했다

개봉 예정 영화 관객수를 예측이 이번 공모전의 주제였다.

그래서 대충 다중회귀로 모델을 만들어서 예측하려고 했다.

그러러면 여러가지 독립변수들이 필요한데 그중 하나로 트위터에서  특정기간(개봉 30일전)동안의 특정단어(영화제목)이 언급된 트윗 개수를 측정하기로 했다.

일단 트위터 API를 이용하여 측정하려 시도했다.

일단 API access 토큰을 받았다! 오예~

https://www.youtube.com/watch?v=pUUxmvvl2FE
여기를 참조하면 어떻게 API access 토큰을 얻을 수 있는지 나와있다.
물론 영어지만 화면그대로 따라하면 된다.

트위터 API를 사용하는 여러가지 라이브러리가 있는데 tweepy를 사용했다!

!pip install tweepy


```python
import tweepy
import pandas as pd
import matplotlib.pyplot as plt
```


```python
from tweepy import Stream
from tweepy import OAuthHandler
from tweepy.streaming import StreamListener

consumer_key ='aJ8sfZ6a7IC7fncSIGKy9YHp3'
consumer_secret = 'NlrLeIKPpwfKzXaJNKpNLM2dXLnxcSJWB6Xr3RYsTvqEfrBlsE'
auth = tweepy.OAuthHandler(consumer_key=consumer_key, consumer_secret=consumer_secret)
api = tweepy.API(auth)

atoken = '909765471217508354-X9qJDspMiPGGE795Y97RDF4aXdXW5uK'
asecret = '909765471217508354-X9qJDspMiPGGE795Y97RDF4aXdXW5uK'

```


```python
results = []
for tweet in tweepy.Cursor(api.search, q='제 7기사단',since='2015-07-01', count=100).items():
    results.append(tweet)
    
print(len(results))
```

    4
    


```python
def process_results(results):
    id_list = [tweet.id for tweet in results]
    data_set = pd.DataFrame(id_list, columns=["id"])

    # Processing Tweet Data

    data_set["text"] = [tweet.text for tweet in results]
    data_set["created_at"] = [tweet.created_at for tweet in results]
    data_set["retweet_count"] = [tweet.retweet_count for tweet in results]
    data_set["favorite_count"] = [tweet.favorite_count for tweet in results]
    data_set["source"] = [tweet.source for tweet in results]

    # Processing User Data
    data_set["user_id"] = [tweet.author.id for tweet in results]
    data_set["user_screen_name"] = [tweet.author.screen_name for tweet in results]
    data_set["user_name"] = [tweet.author.name for tweet in results]
    data_set["user_created_at"] = [tweet.author.created_at for tweet in results]
    data_set["user_description"] = [tweet.author.description for tweet in results]
    data_set["user_followers_count"] = [tweet.author.followers_count for tweet in results]
    data_set["user_friends_count"] = [tweet.author.friends_count for tweet in results]
    data_set["user_location"] = [tweet.author.location for tweet in results]

    return data_set
data_set = process_results(results)
```


```python
data_set.head(10)
```




<div>
<style>
    .dataframe thead tr:only-child th {
        text-align: right;
    }

    .dataframe thead th {
        text-align: left;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>id</th>
      <th>text</th>
      <th>created_at</th>
      <th>retweet_count</th>
      <th>favorite_count</th>
      <th>source</th>
      <th>user_id</th>
      <th>user_screen_name</th>
      <th>user_name</th>
      <th>user_created_at</th>
      <th>user_description</th>
      <th>user_followers_count</th>
      <th>user_friends_count</th>
      <th>user_location</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>909102490460622848</td>
      <td>@Kimsound_ 아 세상에 소리님 제가 아드리앙스승을 설정할때 평민 2기사단이었...</td>
      <td>2017-09-16 17:11:38</td>
      <td>0</td>
      <td>0</td>
      <td>Twitter for Android</td>
      <td>899151272296693765</td>
      <td>178X173_co</td>
      <td>아드리앙 뒤 🐟</td>
      <td>2017-08-20 06:09:03</td>
      <td>참치양 ( @178X173)의 커뮤계입니다! 커뮤이야기를 합니다! 오너님들 저랑 놀...</td>
      <td>29</td>
      <td>35</td>
      <td></td>
    </tr>
    <tr>
      <th>1</th>
      <td>909069578067623936</td>
      <td>RT @TurmiBear: 제 트위터 보시는 분들 이거 끼워맞추기 함 배보죠?\n2...</td>
      <td>2017-09-16 15:00:51</td>
      <td>166</td>
      <td>0</td>
      <td>Twitter for Android</td>
      <td>742668751959523328</td>
      <td>flesruoykcufgo</td>
      <td>[ 반동결 ] 휘모리</td>
      <td>2016-06-14 10:43:23</td>
      <td>1차•잡 그림/커뮤러 | 야행성 | fub free | 소재주의 | 기습마음주의 |...</td>
      <td>271</td>
      <td>290</td>
      <td>.</td>
    </tr>
    <tr>
      <th>2</th>
      <td>908986761082150912</td>
      <td>RT @TurmiBear: 제 트위터 보시는 분들 이거 끼워맞추기 함 배보죠?\n2...</td>
      <td>2017-09-16 09:31:46</td>
      <td>166</td>
      <td>0</td>
      <td>Twitter Web Client</td>
      <td>2323114669</td>
      <td>TurmiBear</td>
      <td>터미베어</td>
      <td>2014-02-02 02:20:02</td>
      <td>중세-르네상스 무술 수련자, 칼덕후, 전근대 덕후.  구호기사단 코스 준비중Asso...</td>
      <td>803</td>
      <td>1873</td>
      <td></td>
    </tr>
    <tr>
      <th>3</th>
      <td>908437392817758208</td>
      <td>RT @8castle: &amp;lt;궁금해 죽겠는데 안풀린 에스티니앙 설정&amp;gt;\n- ...</td>
      <td>2017-09-14 21:08:47</td>
      <td>295</td>
      <td>0</td>
      <td>Twitter for iPhone</td>
      <td>4861643125</td>
      <td>Chimeal_FF14</td>
      <td>🍼🤛😱</td>
      <td>2016-02-05 00:29:12</td>
      <td>카벙클-우유(를때리면)앙팡 / 루비드(♥) / ٩(๑❛ᴗ❛๑)۶</td>
      <td>184</td>
      <td>201</td>
      <td>에기는던전에살아엄마는집에갈거야😑</td>
    </tr>
  </tbody>
</table>
</div>



# 아쉽게도

마케팅에 도움이 될법만한 정보들은 현금을 주고 구할수밖에 없었다.

트위터 API로는 거대한 트윗 데이터들중에서 몇개의 샘플만 얻을 수 있는 구조였다.

# 5분당 180 번의 쿼리만 가능했고 1주일전 트윗까지만 액세스 할 수 있었다. 

본인은 1088개의 영화의 특정 기간동안의 모든 트윗 데이터가 필요했지만

트위터 API를 이용해서는 구할 수 없는 정보였다.

인터넷 검색을 해보니  트위터 데이터베이스 접근권한을 구매한 트위터 써드파티들에게 일정 금액을 지불하고 얻을 수 있는 정보들이었다.

물론 가격은 기본이 백만원단위 였다.

그래서 API를 이용치 않고 직접 크롤러를 제작하려고 시도했다.

https://beomi.github.io/2017/01/20/HowToMakeWebCrawler/

위는 웹크롤러 만들기가 잘 설명되어있는 웹사이트!

# 하지만 트위터는

동적 웹크롤러가 필요한 싸이트였다.

트위터 싸이트에 접속하여 고급검색을 통해서는 과거 특정시점의 트윗 조회가 가능했지만

스크롤 바를 내리면 그전 트윗들이 갱신되는 방식으로 검색결과가 조회되었기에

기본적은 웹크롤러로는 모든 트윗 정보를 얻어올수가 없는 구조였다.


# 고민하던 와중 Github에서 말도 안되는 정보를 얻게 되었다.


![image.png](attachment:image.png)

누군가가 트위터 크롤러를 만들어 둔 것이다.

심지어 API를 사용치않고 트위터 고급검색 웹페이지에 직접 접근하여 모든 트윗을 가져오는 방식으로 

5분당 180쿼리 제한도 없고 일주일전이아니라 모든 과거정보를 접근할 수 있는 신비의 코드였다.

이 서양에서 온 은자는 https://github.com/taspinar/twitterscraper 이 주소를 통해 만날수 있다.

그 트위터 웹 크롤러의 자세한 코드 내용은 아래와 같다


```python
# TwitterScraper
# Copyright 2016-2017 Ahmet Taspinar
# See LICENSE for details.
"""
Twitter Scraper tool
"""

__version__ = '0.1'
__author__ = 'Ahmet Taspinar'
__license__ = 'MIT'


from twitterscraper.query import query_tweets
from twitterscraper.tweet import Tweet
```


```python
"""
This is a command line application that allows you to scrape twitter!
"""
import collections
import json
from argparse import ArgumentParser
from datetime import datetime
from os.path import isfile
from json import dump
import logging

from twitterscraper import query_tweets
from twitterscraper.query import query_all_tweets


class JSONEncoder(json.JSONEncoder):

    def default(self, obj):
        if hasattr(obj, '__json__'):
            return obj.__json__()
        elif isinstance(obj, collections.Iterable):
            return list(obj)
        elif isinstance(obj, datetime):
            return obj.isoformat()
        elif hasattr(obj, '__getitem__') and hasattr(obj, 'keys'):
            return dict(obj)
        elif hasattr(obj, '__dict__'):
            return {member: getattr(obj, member)
                    for member in dir(obj)
                    if not member.startswith('_') and
                    not hasattr(getattr(obj, member), '__call__')}

        return json.JSONEncoder.default(self, obj)


def main():
    logging.basicConfig(format='%(levelname)s: %(message)s', level=logging.INFO)
    try:
        parser = ArgumentParser(
            description=__doc__
        )

        parser.add_argument("query", type=str, help="Advanced twitter query")
        parser.add_argument("-o", "--output", type=str, default="tweets.json",
                            help="Path to a JSON file to store the gathered "
                                 "tweets to.")
        parser.add_argument("-l", "--limit", type=int, default=None,
                            help="Number of minimum tweets to gather.")
        parser.add_argument("-a", "--all", action='store_true',
                            help="Set this flag if you want to get all tweets "
                                 "in the history of twitter. This may take a "
                                 "while but also activates parallel tweet "
                                 "gathering. The number of tweets however, "
                                 "will be capped at around 100000 per 10 "
                                 "days.")
        args = parser.parse_args()

        if isfile(args.output):
            logging.error("Output file already exists! Aborting.")
            exit(-1)

        if args.all:
            tweets = query_all_tweets(args.query)
        else:
            tweets = query_tweets(args.query, args.limit)

        with open(args.output, "w") as output:
            dump(tweets, output, cls=JSONEncoder)
    except KeyboardInterrupt:
        logging.info("Program interrupted by user. Quitting...")

```


```python
import json
import logging
import random
import sys
from datetime import timedelta, date
from multiprocessing.pool import Pool

import requests
from fake_useragent import UserAgent
from twitterscraper.tweet import Tweet


ua = UserAgent()
HEADERS_LIST = [ua.chrome, ua.google, ua['google chrome'], ua.firefox, ua.ff]

INIT_URL = "https://twitter.com/search?f=tweets&vertical=default&q={q}"
RELOAD_URL = "https://twitter.com/i/search/timeline?f=tweets&vertical=" \
             "default&include_available_features=1&include_entities=1&" \
             "reset_error_state=false&src=typd&max_position={pos}&q={q}"


def query_single_page(url, html_response=True, retry=3):
    """
    Returns tweets from the given URL.

    :param url: The URL to get the tweets from
    :param html_response: False, if the HTML is embedded in a JSON
    :param retry: Number of retries if something goes wrong.
    :return: The list of tweets, the pos argument for getting the next page.
    """
    headers = {'User-Agent': random.choice(HEADERS_LIST)}

    try:
        response = requests.get(url, headers=headers)
        if html_response:
            html = response.text
        else:
            json_resp = response.json()
            html = json_resp['items_html']

        tweets = list(Tweet.from_html(html))

        if not tweets:
            return [], None

        if not html_response:
            return tweets, json_resp['min_position']

        return tweets, "TWEET-{}-{}".format(tweets[-1].id, tweets[0].id)
    except requests.exceptions.HTTPError as e:
        logging.exception('HTTPError {} while requesting "{}"'.format(
            e, url))
    except requests.exceptions.ConnectionError as e:
        logging.exception('ConnectionError {} while requesting "{}"'.format(
            e, url))
    except requests.exceptions.Timeout as e:
        logging.exception('TimeOut {} while requesting "{}"'.format(
            e, url))
    if retry > 0:
        logging.info("Retrying...")
        return query_single_page(url, html_response, retry-1)

    logging.error("Giving up.")
    return [], None


def query_tweets_once(query, limit=None, num_tweets=0):
    """
    Queries twitter for all the tweets you want! It will load all pages it gets
    from twitter. However, twitter might out of a sudden stop serving new pages,
    in that case, use the `query_tweets` method.

    Note that this function catches the KeyboardInterrupt so it can return
    tweets on incomplete queries if the user decides to abort.

    :param query: Any advanced query you want to do! Compile it at
                  https://twitter.com/search-advanced and just copy the query!
    :param limit: Scraping will be stopped when at least ``limit`` number of
                  items are fetched.
    :param num_tweets: Number of tweets fetched outside this function.
    :return:      A list of twitterscraper.Tweet objects. You will get at least
                  ``limit`` number of items.
    """
    logging.info("Querying {}".format(query))
    query = query.replace(' ', '%20').replace("#", "%23").replace(":", "%3A")
    pos = None
    tweets = []
    try:
        while True:
            new_tweets, pos = query_single_page(
                INIT_URL.format(q=query) if pos is None
                else RELOAD_URL.format(q=query, pos=pos),
                pos is None
            )
            if len(new_tweets) == 0:
                logging.info("Got {} tweets for {}.".format(
                    len(tweets), query))
                return tweets

            logging.info("Got {} tweets ({} new).".format(
                len(tweets) + num_tweets, len(new_tweets)))

            tweets += new_tweets

            if limit is not None and len(tweets) + num_tweets >= limit:
                return tweets
    except KeyboardInterrupt:
        logging.info("Program interrupted by user. Returning tweets gathered "
                     "so far...")
    except BaseException:
        logging.exception("An unknown error occurred! Returning tweets "
                          "gathered so far.")

    return tweets


def eliminate_duplicates(iterable):
    """
    Yields all unique elements of an iterable sorted. Elements are considered
    non unique if the equality comparison to another element is true. (In those
    cases, the set conversion isn't sufficient as it uses identity comparison.)
    """
    class NoElement: pass

    prev_elem = NoElement
    for elem in sorted(iterable):
        if prev_elem is NoElement:
            prev_elem = elem
            yield elem
            continue

        if prev_elem != elem:
            prev_elem = elem
            yield elem


def query_tweets(query, limit=None):
    tweets = []
    iteration = 1

    while limit is None or len(tweets) < limit:
        logging.info("Running iteration no {}, query is {}".format(
            iteration, repr(query)))
        new_tweets = query_tweets_once(query, limit, len(tweets))
        tweets.extend(new_tweets)

        if not new_tweets:
            break

        mindate = min(map(lambda tweet: tweet.timestamp, new_tweets)).date()
        maxdate = max(map(lambda tweet: tweet.timestamp, new_tweets)).date()
        logging.info("Got tweets ranging from {} to {}".format(
            mindate.isoformat(), maxdate.isoformat()))

        # Add a day, twitter only searches until excluding that day and we dont
        # have complete results for that one yet. However, we cannot limit the
        # search to less than one day: if all results are from the same day, we
        # want to continue searching further into the past: either there are no
        # further results or twitter stopped serving them and there's nothing
        # we can do.
        if mindate != maxdate:
            mindate += timedelta(days=1)

        # Twitter will always choose the more restrictive until:
        query += ' until:' + mindate.isoformat()
        iteration += 1

    # Eliminate duplicates
    return list(eliminate_duplicates(tweets))


def query_all_tweets(query):
    """
    Queries *all* tweets in the history of twitter for the given query. This
    will run in parallel for each ~10 days.

    :param query: A twitter advanced search query.
    :return: A list of tweets.
    """
    year = 2006
    month = 3

    limits = []
    while date(year=year, month=month, day=1) < date.today():
        nextmonth = month + 1 if month < 12 else 1
        nextyear = year + 1 if nextmonth == 1 else year

        limits.append(
            (date(year=year, month=month, day=1),
             date(year=year, month=month, day=10))
        )
        limits.append(
            (date(year=year, month=month, day=10),
             date(year=year, month=month, day=20))
        )
        limits.append(
            (date(year=year, month=month, day=20),
             date(year=nextyear, month=nextmonth, day=1))
        )
        year, month = nextyear, nextmonth

    queries = ['{} since:{} until:{}'.format(query, since, until)
               for since, until in reversed(limits)]

    pool = Pool(20)
    all_tweets = []
    try:
        for new_tweets in pool.imap_unordered(query_tweets_once, queries):
            all_tweets.extend(new_tweets)
            logging.info("Got {} tweets ({} new).".format(
                len(all_tweets), len(new_tweets)))
    except KeyboardInterrupt:
        logging.info("Program interrupted by user. Returning all tweets "
                     "gathered so far.")

    return sorted(all_tweets)

```


```python
from datetime import datetime

from bs4 import BeautifulSoup
from coala_utils.decorators import generate_ordering


@generate_ordering('timestamp', 'id', 'text', 'user', 'replies', 'retweets', 'likes')
class Tweet:
    def __init__(self, user, id, timestamp, fullname, text, replies, retweets, likes):
        self.user = user
        self.id = id
        self.timestamp = timestamp
        self.fullname = fullname
        self.text = text
        self.replies = replies
        self.retweets = retweets
        self.likes = likes

    @classmethod
    def from_soup(cls, tweet):
        return cls(
            user=tweet.find('span', 'username').text[1:],
            id=tweet['data-item-id'],
            timestamp=datetime.utcfromtimestamp(
                int(tweet.find('span', '_timestamp')['data-time'])),
            fullname=tweet.find('strong', 'fullname').text,
            text=tweet.find('p', 'tweet-text').text or "",
            replies = tweet.find('div', 'ProfileTweet-action--reply').find('span', 'ProfileTweet-actionCountForPresentation').text or '0',
            retweets = tweet.find('div', 'ProfileTweet-action--retweet').find('span', 'ProfileTweet-actionCountForPresentation').text or '0',
            likes = tweet.find('div', 'ProfileTweet-action--favorite').find('span', 'ProfileTweet-actionCountForPresentation').text or '0'
        )

    @classmethod
    def from_html(cls, html):
        soup = BeautifulSoup(html, "lxml")
        tweets = soup.find_all('li', 'js-stream-item')
        if tweets:
            for tweet in tweets:
                try:
                    yield cls.from_soup(tweet)
                except AttributeError:
                    pass  # Incomplete info? Discard!

```

# 도저히 내 실력으로는 흉내낼수 없는 간결한 코드였다.

이 코드를 바탕으로 실제로 영화 데이터를 수집했다.


```python
import pandas as pd
import numpy as np
```


```python
df_movie= pd.read_csv('people.csv',encoding='cp949')
```


```python
df_movie.head(504)
```


```python
name = df_movie.영화명
date1 = df_movie.개봉일
date2 = df_movie.개봉2일후
date3 = df_movie.개봉28일전
date4 = df_movie.개봉4일전
date5 = df_movie.개봉34일전
prediction1 = df_movie.단어영화포함트윗수1
prediction2 = df_movie.단어영화포함라잌수1
prediction3 = df_movie.트윗수1
prediction4 = df_movie.라잌수1
prediction21 = df_movie.단어영화포함트윗수2
prediction22 = df_movie.단어영화포함라잌수2
prediction23 = df_movie.트윗수2
prediction24 = df_movie.라잌수2
```


```python
filename = 'people.csv'
```


```python
a = 515
filename = 'people.csv'

while (a<752):
    a = a + 1
    print(a)
    query = ""+name[a]+" since:"+str(date3[a].replace(" ","-"))+" until:"+str(date2[a].replace(" ","-"))
    counttweet = 0
    countlikes = 0
    for tweet in query_tweets_once(query):
        counttweet = counttweet + 1 + int(tweet.retweets.replace(",",""))
        countlikes = countlikes +int(tweet.likes.replace(",",""))
    prediction3[a] = counttweet
    prediction4[a] = countlikes
    query = ""+name[a]+" since:"+str(date5[a].replace(" ","-"))+" until:"+str(date4[a].replace(" ","-"))
    counttweet = 0
    countlikes = 0
    for tweet in query_tweets_once(query):
        counttweet = counttweet + 1 + int(tweet.retweets.replace(",",""))
        countlikes = countlikes +int(tweet.likes.replace(",",""))
    prediction23[a] = counttweet
    prediction24[a] = countlikes
    df_movie.to_csv(filename, index=False, encoding='cp949')
```



각 영화의 개봉 4일전~개봉34일전 개봉2일후~개봉28일전 동안 그 특정 영화가 회자된 트윗개수와 그 트윗이 라이크를 받은 횟수를 계산하여 저장했다

물론 데이터를 얻는데 시간은 꽤 오래 걸렸다. (직접 웹사이트에 접속해 정보를 얻어오는 방식이었기에..)


하지만 금액 지불없이 이 모든걸 공짜로 할 수 있었다.

# 느낀점이 크다.

사실 처음으로 오픈소스의 힘을 느꼈다.

만약 내가 깃허브를 몰랐고 파이썬을 전혀몰랐다면 영락없이 수십만원을 지불할수밖에 없었겠지만

몇번의 검색으로 (사실 하루가 꼬박 걸렸지만) 내가 원하는 데이터를 정보의 바다에서 건져 올릴 수 있었다.

# 물론 내가 직접 코드를 짜는 것도 중요하지만

세상엔 이미 너무 좋은 코드들이 나와있다. 

그걸 잘 찾아서 이용하는게 더 효과적인듯 싶다.

이번 주는 당면한 과제인 공모전이 너무 급해 일단 공부가 아니라 해결방법에 시간을 쏟아 부었다.

## 다음주부턴 밑바닥부터 시작하는 데이터 사이언스 1장부터 차근차근 시작해야겠다.

# 이번주 끝.
