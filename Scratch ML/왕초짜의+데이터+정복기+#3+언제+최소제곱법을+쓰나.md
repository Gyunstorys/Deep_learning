
# 저번에 시도한 빅콘테스트는 참패했다..

# 잘못한게 너무나도 많아서 하나하나 소개하기도 벅차다

# 그 중 가장 중요한 estimator에 대해 조금 설명하고자 한다. 그에 앞서..

데이터 정복기의 스타일에 변화를 주기로 했다.

내가 일주일간 배운 내용을 모두 적기에는 의미가 없다고 생각했다.

차라리 내가 적은 내용을 보기보다 교재를 직접 사서 읽는게 나을 것이라 판단했다. (교재도 번역본이고 훨씬 잘 쓰여져 있다.)

그래서 배운 내용중 가장 의미있는 내용만을 담아 3분안에 읽을 수 있는 게시글을 제작하고자 한다!

이번 주에 가장 의미있었던 내용은... 어떤 경우에 오차계산방법으로 최소제곱법을 써야하는지 소개하려 한다.

보통 나같은 초짜들은 아무생각없이 최소제곱법(OLS)를 사용하는데 이건 큰 오산이다. 

OLS가 계산하고 쓰기 쉬워서 맨처음 배우게 되는 방법인데 이 방법을 의미가 있으려면 다음의 가정이 필요하다.



# 가정 1. 독립변수값은 고정된 값이어야 한다.

무슨 말이냐하면 독립변수의 값들은 때에 따라 임의로 변하는 값(Random Variable)이 아니라 측정할때마다 똑같은 값을 주는 고정된 값(fixed value)여야 한다.

# 가정 2. 실제 오차의 예측값은 0이어야 한다.

E(et) = 0 이라는 의미, 다시 말하자면 실제 오차(사실상 측정이 불가능하다)의 평균값은 0 이어야한다. 여기서부터 참으로 말도 안되는 가정의 시작이다. 여기서 말하는 오차값은 표본들의 오차값이 아니라 우리가 알지 못하는 모집단의 오차값을 의미한다. 

# 가정 3. 실제 오차값의 분산은 독립변수과 상관없이 같아야 한다.

모집단에서 설명변수에 대한 오차값의 분산가 같아야 한다는말. 모집단에는 y1 y2 y3 y4 ..... Yt까지 무수하다 이 모든 설명변수들의 오차값의 분산이 독립변수에 상관없이 모두 같아야한다는 가정

# 가정 4. 설명변수들의 오차들의 공분산은 0이어야 한다.

다시 말하자면 y1의 오차 e1와 y2의 오차 e2의 공분산인 cov(e1,e2)는 0이어야한다. 이것도 실제 벌어지기 힘든 가정중에 하나다.

이러한 4가지 가정이 실제하면 가우스마가로브 정리에 의해 최소제곱법이 BLUE(Best Linear Unbiased Estimator)가 된다 

이 외에도 두가지 가정이 더있는데 이런 가정들을 CLRM(Classical linear regression model) assumptions 라고 부른다.

사실 이 가정들이 모두 사실인 경우는 매우 희귀할 뿐더러 이 가정들이 사실인지 확인하기고 힘들다.

하지만 최소제곱법은 아주 강력한 estimator들 중 하나 인건 확실하다. 그렇지만 최소제곱법이 특정 조건을 만족시키는 경우에 더욱 강력해진다.

오차를 계산하는 estimator를 선택할때 꼭 이런 요소들을 염두해두길 바란다.

사실... 필자도 뭐가 뭔지 하나도 모르겠다. 대충 이런게 있다는 정도만 알게되었고 더욱 더 공부해봐야겠다.

# 끝
