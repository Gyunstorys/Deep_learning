{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Training \n",
    "\n",
    "RNN, LSTM Training시 문제가 되는 두가지 문제 : <br>\n",
    "* Vanishing gradient <br>\n",
    "* Exploding gradient <br>\n",
    "\n",
    "간단히 말해서, hidden layer에 tanh를 씌우면 미분값이 1보다 작기에 **vanishing gradient** 문제, <br>\n",
    "relu 등 미분값이 1이상인 activation function의 경우 **exploding gradient**문제가 있다. <br>\n",
    "\n",
    "Vanishing gradient의 문제의 해결방식은 다음과 같이 매우 심도있게 논의되어 왔다 : <br>\n",
    "\n",
    "* 이전 hidden layer에서 들어오는 값의 계수 행렬 W의 초기값을 잘 정해준다. <br>\n",
    "\n",
    "* W의 regularization을 잘 정해줘도 비슷한 효과를 볼 수 있다.<br>\n",
    "예)$ w^* = argmin_w \\Sigma_j(t(x_j) - \\Sigma_iw_ih_i(x_j))^2 + \\lambda \\Sigma^k |w_i|$\n",
    "* tanh나 sigmoid activation 함수 말고 ReLU를 사용. <br>\n",
    "\n",
    "* Long Short-Term Memory (LSTM)이나 Gated Recurrent Unit (GRU) 구조를 사용하는 방법 - **가장 인기** <br>\n",
    "\n",
    "그렇다면 exploding gradient는 왜 해결에 주목받지 못하는가? <br>\n",
    "\n",
    "이유는 vanishing gradient는 더 detecting이 어렵고, exploding의 경우에는 잘 보인다는 점도 있지만, <br>\n",
    "근본적으로 해결책이 완성되었기 때문이다 : [**gradient norm clipping stategy**](https://arxiv.org/pdf/1211.5063.pdf)\n",
    "\n",
    "## Gradient norm clipping stategy란\n",
    "\n",
    "ps)\n",
    "이를 논하기에 앞서 먼저 RNN의 여러 형태를 바라보자. :\n",
    "\n",
    "* $ s_t = tanh(Ux_t + Ws_{t-1}) $ <br>\n",
    "* $ x_t = W_{rec}\\sigma (x_{t-1}) + W_{in}u_t + b $ where $\\sigma $ is tanh or sigmoid<br>\n",
    "\n",
    "이렇게 다양한 방식으로 activation function을 포함한 RNN 표현 방식이 있음을 알고 넘어가야한다. <br>\n",
    "\n",
    "본 설명은 편의에 의해 (2)번 설명으로만 포커싱을 함을 유념한다\n",
    "\n",
    "### BPTT(Backpropagation Through Time)\n",
    "\n",
    "![bptt 그림](http://www.wildml.com/wp-content/uploads/2015/10/rnn-bptt-with-gradients.png)\n",
    "\n",
    "$ s_t = tanh(Ux_t + Ws_{t-1}) $ <br>\n",
    "$ \\hat{y_t} = softmax(Vs_t) $ <br>\n",
    "$E_t=-y_t log \\hat{y_t}$ <br>\n",
    "$ \\dfrac{\\partial E_3}{W} = \\Sigma^3 \\dfrac{\\partial E_3}{\\partial \\hat{y_3}} \\dfrac{\\partial \\hat{y_3}}{\\partial s_3} \\dfrac{\\partial s_3}{\\partial s_k} \\dfrac{\\partial s_k}{\\partial W}$<br>\n",
    "\n",
    "2번에 의거 $\\dfrac{\\partial s_3}{\\partial s_k} < 1 $ <br>\n",
    "\n",
    "따라서 이 파트가 너무 작다면 vanishing grad, <br>\n",
    "이 파트가 적정선인데 $\\dfrac{\\partial s_k}{\\partial W}$ 본 파트가 1보다 훨씬 크면  exploding 이다. <br>\n",
    "\n",
    "\n",
    "### previous 해결책\n",
    "L1, L2 for $ W_{rec}$ is so well known solution <br>\n",
    "\n",
    "### Gradient norm clipping > pseudo code\n",
    "whenever\n",
    "if $ || W_{rec} || \\geq threshold$ then <br>\n",
    "    $W_{rec} \\leftarrow \\dfrac{threshold}{||\\hat{g}||}\\hat{g} $<br>"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:torch]",
   "language": "python",
   "name": "conda-env-torch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
