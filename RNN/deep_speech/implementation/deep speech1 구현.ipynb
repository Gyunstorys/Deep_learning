{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DEEP SPEECH1 구현 - pytorch\n",
    "\n",
    "CTC loss 를 이용한 deep speech 구현이다. <br>\n",
    "구현 모델은 [Deep speech 이론 by YBigTa](https://github.com/YBIGTA/Deep_learning/blob/master/RNN/deep%20speech/%EC%84%A4%EB%AA%85/Deep%20speech_%EC%83%81%ED%97%8C.pdf) 에서 확인 가능하다. <br>\n",
    "\n",
    "본 코드는 [deep speech2 implementation](https://github.com/SeanNaren/deepspeech.pytorch/)을 상당부분 참고한다.\n",
    "\n",
    "### 구현 stack\n",
    "OS : ubuntu 16.04 <br>\n",
    "conda : 4.2.9 <br>\n",
    "\n",
    "\n",
    "## 설치\n",
    "\n",
    "오디오 I/O 를 위한 pytorch audio를 설치한다. <br>\n",
    "```\n",
    "sudo apt-get install sox libsox-dev libsox-fmt-all\n",
    "git clone https://github.com/pytorch/audio.git\n",
    "cd audio\n",
    "python setup.py install\n",
    "```\n",
    "\n",
    "Requirments를 설치한다. <br>\n",
    "```\n",
    "sudo pip install python-levenshtein torch visdom wget librosa\n",
    "```\n",
    "\n",
    "## 0. 데이터셋 다운로드\n",
    "\n",
    "데이터셋의 경우 카네기 멜론 대학교에서 제공한 free dataset인 **AN4** 를 사용한다.\n",
    "\n",
    "```python\n",
    "import argparse\n",
    "import os\n",
    "import io\n",
    "import shutil\n",
    "import tarfile\n",
    "import wget\n",
    "\n",
    "import subprocess\n",
    "\n",
    "def _order_files(file_paths):\n",
    "    print(\"Sorting files by length...\")\n",
    "\n",
    "    def func(element):\n",
    "        output = subprocess.check_output(\n",
    "            ['soxi -D \\\"%s\\\"' % element.strip()],\n",
    "            shell=True\n",
    "        )\n",
    "        return float(output)\n",
    "\n",
    "    file_paths.sort(key=func)\n",
    "    \n",
    "def create_manifest(data_path, tag, ordered=True):\n",
    "    manifest_path = '%s_manifest.csv' % tag\n",
    "    file_paths = []\n",
    "    wav_files = [os.path.join(dirpath, f)\n",
    "                 for dirpath, dirnames, files in os.walk(data_path)\n",
    "                 for f in fnmatch.filter(files, '*.wav')]\n",
    "    size = len(wav_files)\n",
    "    counter = 0\n",
    "    for file_path in wav_files:\n",
    "        file_paths.append(file_path.strip())\n",
    "        counter += 1\n",
    "    print('\\n')\n",
    "    if ordered:\n",
    "        _order_files(file_paths)\n",
    "    counter = 0\n",
    "    with io.FileIO(manifest_path, \"w\") as file:\n",
    "        for wav_path in file_paths:\n",
    "            transcript_path = wav_path.replace('/wav/', '/txt/').replace('.wav', '.txt')\n",
    "            sample = os.path.abspath(wav_path) + ',' + os.path.abspath(transcript_path) + '\\n'\n",
    "            file.write(sample.encode('utf-8'))\n",
    "            counter += 1\n",
    "    print('\\n')\n",
    "\n",
    "# command line에서 동작하는 것들을 더 쉽게 만들어주는 parser를 이용하여 데이터를 다운받는다.\n",
    "parser = argparse.ArgumentParser(description='Processes and downloads an4.') \n",
    "parser.add_argument('--target_dir', default='an4_dataset/', help='Path to save dataset')\n",
    "parser.add_argument('--sample_rate', default=16000, type=int, help='Sample rate')\n",
    "args = parser.parse_args()\n",
    "\n",
    "\n",
    "def _format_data(root_path, data_tag, name, wav_folder):\n",
    "    data_path = args.target_dir + data_tag + '/' + name + '/'\n",
    "    new_transcript_path = data_path + '/txt/'\n",
    "    new_wav_path = data_path + '/wav/'\n",
    "\n",
    "    os.makedirs(new_transcript_path)\n",
    "    os.makedirs(new_wav_path)\n",
    "\n",
    "    wav_path = root_path + 'wav/'\n",
    "    file_ids = root_path + 'etc/an4_%s.fileids' % data_tag\n",
    "    transcripts = root_path + 'etc/an4_%s.transcription' % data_tag\n",
    "    train_path = wav_path + wav_folder\n",
    "\n",
    "    _convert_audio_to_wav(train_path)\n",
    "    _format_files(file_ids, new_transcript_path, new_wav_path, transcripts, wav_path)\n",
    "\n",
    "\n",
    "def _convert_audio_to_wav(train_path):\n",
    "    with os.popen('find %s -type f -name \"*.raw\"' % train_path) as pipe:\n",
    "        for line in pipe:\n",
    "            raw_path = line.strip()\n",
    "            new_path = line.replace('.raw', '.wav').strip()\n",
    "            cmd = 'sox -t raw -r %d -b 16 -e signed-integer -B -c 1 \\\"%s\\\" \\\"%s\\\"' % (\n",
    "                args.sample_rate, raw_path, new_path)\n",
    "            os.system(cmd)\n",
    "\n",
    "\n",
    "def _format_files(file_ids, new_transcript_path, new_wav_path, transcripts, wav_path):\n",
    "    with open(file_ids, 'r') as f:\n",
    "        with open(transcripts, 'r') as t:\n",
    "            paths = f.readlines()\n",
    "            transcripts = t.readlines()\n",
    "            for x in range(len(paths)):\n",
    "                path = wav_path + paths[x].strip() + '.wav'\n",
    "                filename = path.split('/')[-1]\n",
    "                extracted_transcript = _process_transcript(transcripts, x)\n",
    "                current_path = os.path.abspath(path)\n",
    "                new_path = new_wav_path + filename\n",
    "                text_path = new_transcript_path + filename.replace('.wav', '.txt')\n",
    "                with io.FileIO(text_path, \"w\") as file:\n",
    "                    file.write(extracted_transcript.encode('utf-8'))\n",
    "                os.rename(current_path, new_path)\n",
    "\n",
    "\n",
    "def _process_transcript(transcripts, x):\n",
    "    extracted_transcript = transcripts[x].split('(')[0].strip(\"<s>\").split('<')[0].strip().upper()\n",
    "    return extracted_transcript\n",
    "\n",
    "\n",
    "def main():\n",
    "    root_path = 'an4/'\n",
    "    name = 'an4'\n",
    "    wget.download('http://www.speech.cs.cmu.edu/databases/an4/an4_raw.bigendian.tar.gz')\n",
    "    tar = tarfile.open('an4_raw.bigendian.tar.gz')\n",
    "    tar.extractall()\n",
    "    os.makedirs(args.target_dir)\n",
    "    _format_data(root_path, 'train', name, 'an4_clstk')\n",
    "    _format_data(root_path, 'test', name, 'an4test_clstk')\n",
    "    shutil.rmtree(root_path)\n",
    "    os.remove('an4_raw.bigendian.tar.gz')\n",
    "    train_path = args.target_dir + '/train/'\n",
    "    test_path = args.target_dir + '/test/'\n",
    "    print ('\\n', 'Creating manifests...')\n",
    "    create_manifest(train_path, 'an4_train')\n",
    "    create_manifest(test_path, 'an4_val')\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Making audio spectogram using fourier transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset label csv 파일 제작\n",
    "\n",
    "### training, testing을 위한 data는 다음과 같은 형태로 폴더 내에 저장이 되어 있다. \n",
    "\n",
    "#### training \n",
    "\n",
    "*data/data_path_to_train/input* <br>\n",
    "*data/data_path_to_train/label* <br>\n",
    "\n",
    "#### testing\n",
    "\n",
    "*data/data_path_to_test/input* <br>\n",
    "*data/data_path_to_test/label* <br>\n",
    "\n",
    "따라서 본 데이터를 load 하기 위하여 training 시, testing 시 각각 불러올 파일들의 path를 하나의 csv파일(manifest라 부른다.) 로 저장하면 부르기에 용이하다. <br>\n",
    "\n",
    "위의 코드 중 create_manifest 함수가 본 파일을 csv화 해주는 함수이다. <br>\n",
    "\n",
    "따라서 코드를 돌린 장소에 csv 파일을 참조한다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset과 Dataloader 구현\n",
    "\n",
    "pytorch의 dataset과 dataloader의 경우는 그 클래스가 정해져 있다. <br>\n",
    "\n",
    "따라서 data loader와 dataset의 경우에는 사용자 정의 클래스를 만들기 위해서 **torch.utils.data**의 **Dataset**과 **DataLoader**를 상속받아야 한다. <br>\n",
    "\n",
    "이를 통해 사용자 정의 dataset, dataloader를 구현할 것이며, <br>\n",
    "\n",
    "본 dataset에는 audio spectogram 형태의 dataset이여야 하므로 *wav* 파일을 *spectogram* 형태로 바꾸는 함수를 집어넣고 구현한다., <br>\n",
    "\n",
    "추가적인 dataset, dataloader 구현 튜토리얼은 [pytorch 공인 튜토리얼](http://pytorch.org/tutorials/beginner/data_loading_tutorial.html) 을 참고한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 필요 class load\n",
    "import librosa\n",
    "import numpy as np\n",
    "import scipy.signal\n",
    "import torch\n",
    "import torchaudio\n",
    "import copy\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "wav 파일을 load하는 함수.\n",
    "torchaudio를 사용한다. \n",
    "\n",
    "Input : \n",
    "    wav 파일 path. 자료형 : str\n",
    "Output :\n",
    "    오디오 파일의 numpy 형태. 자료형 : np\n",
    "\"\"\"\n",
    "def load_audio(path):\n",
    "    sound, _ = torchaudio.load(path)\n",
    "    sound = sound.numpy()\n",
    "    if len(sound.shape) > 1:\n",
    "        if sound.shape[1] == 1:\n",
    "            sound = sound.squeeze()\n",
    "        else:\n",
    "            sound = sound.mean(axis=1)  # multiple channels, average\n",
    "    return sound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Fourier transformation parser.\n",
    "spectogramparser는 spectogramDataset에 상속되며, 모든 wav 파일을 spectogram으로 파싱하는 클래스이다.\n",
    "\n",
    "Param : \n",
    "    audio_conf - 오디오 특성의 딕셔너리. spectogram화를 위한 window 방식, window size, stride, 음성의 rate 가 들어있어야 한다. 자료형 : dict\n",
    "\"\"\"\n",
    "class SpectogramParser(object):\n",
    "    # 초기화 함수. \n",
    "    def __init__(self, audio_conf):\n",
    "        super(SpectogramParser, self).__init__()\n",
    "        self.window_stride = audio_conf['window_stride']\n",
    "        self.window_size = audio_conf['window_size']\n",
    "        self.sample_rate = audio_conf['sample_rate']\n",
    "        self.window = audio_conf['window']\n",
    "    \n",
    "    \"\"\"\n",
    "    parsing 함수. \n",
    "    Input : \n",
    "        audio_path. 자료형 : str or list\n",
    "    output : \n",
    "        해당 path의 spectogram, 자료형 : FloatTensor (MHz + 1, len)\n",
    "    \"\"\"\n",
    "    def parse_audio(self, audio_path):\n",
    "        # 하나의 path가 string형태로 들어온다면\n",
    "        if(type(audio_path)==str):\n",
    "            y = load_audio(audio_path)\n",
    "            n_fft = int(self.sample_rate * self.window_size)\n",
    "            win_length = n_fft\n",
    "            hop_length = int(self.sample_rate * self.window_stride)\n",
    "            # STFT\n",
    "            D = librosa.stft(y, n_fft=n_fft, hop_length=hop_length,\n",
    "                             win_length=win_length, window=self.window)\n",
    "            spect, phase = librosa.magphase(D)\n",
    "            # S = log(S+1)\n",
    "            spect = np.log1p(spect)\n",
    "            spect = torch.FloatTensor(spect)\n",
    "            return spect\n",
    "        # 굳이 필요 없음이 밝혀짐.\n",
    "        # 여러개의 path가 list형태로 들어온다면 tmp에 저장\n",
    "#        elif(type(audio_path)==list):\n",
    "#            tmp = \"\"\n",
    "#            for path in audio_path:\n",
    "#                y = load_audio(path)\n",
    "#                n_fft = int(self.sample_rate * self.window_size)\n",
    "#                win_length = n_fft\n",
    "#                hop_length = int(self.sample_rate * self.window_stride)\n",
    "                # STFT\n",
    "#                D = librosa.stft(y, n_fft=n_fft, hop_length=hop_length,\n",
    "#                                 win_length=win_length, window=self.window)\n",
    "#                spect, phase = librosa.magphase(D)\n",
    "                # S = log(S+1)\n",
    "#                spect = np.log1p(spect)\n",
    "#                shpe = spect.shape\n",
    "#                spect = spect.reshape((1,)+shpe)\n",
    "#                if(tmp == \"\"):\n",
    "#                    tmp = copy.deepcopy(spect)\n",
    "#                else:\n",
    "#                    tmp = np.append(tmp,spect,axis=0)\n",
    "#                \n",
    "#            spect = torch.FloatTensor(tmp)\n",
    "#            return spect               \n",
    "        else:\n",
    "            print(\"wrong type of audiopath\")\n",
    "            return -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *해당 클래스 example*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 17.1550  18.0746  17.1650  ...   16.8284  15.2061  16.6281\n",
       " 15.7563  17.7952  16.6147  ...   17.0611  17.5940  16.2806\n",
       " 15.8626  17.1986  16.3381  ...   16.0992  17.9769  14.8790\n",
       "           ...               ⋱              ...            \n",
       " 16.2552  14.8504  16.8628  ...   15.0124  14.9490  15.1515\n",
       " 16.0438  15.2556  15.3839  ...   15.0961  14.9769  15.6644\n",
       " 15.8748  14.8320  15.4217  ...   14.0025  14.5311  15.2816\n",
       "[torch.FloatTensor of size 161x151]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sample.wav 파일로 테스트. 본 파일은 an4 training set의 첫 번째 data이다.\n",
    "audio_conf = {}\n",
    "audio_conf[\"sample_rate\"] = torchaudio.load(\"./sample.wav\")[1] #torchaudio의 2번째 return value는 해당 wav 파일의 rate이다.\n",
    "audio_conf[\"window_size\"] = 0.02\n",
    "audio_conf[\"window_stride\"] = 0.01\n",
    "audio_conf[\"window\"] = scipy.signal.hamming\n",
    "\n",
    "#class 초기화\n",
    "parser =SpectogramParser(audio_conf)\n",
    "\n",
    "#parsing된 데이터\n",
    "parser.parse_audio(\"./sample.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "  9.9548  12.5342  12.5185  ...   21.6904  21.7963  21.1424\n",
       "  9.9540  12.6197  13.0785  ...   23.0774  21.1615  22.5326\n",
       "  9.9516  12.7327  12.4498  ...   23.8243  23.5357  22.5847\n",
       "           ...               ⋱              ...            \n",
       "  1.5135  11.5833  11.7393  ...   15.0627  16.1645  16.5222\n",
       "  2.5937  11.3482  11.6363  ...   15.1313  16.1363  16.5455\n",
       "  2.7930  11.0813  12.4409  ...   15.2082  16.0953  16.4942\n",
       "[torch.FloatTensor of size 442x11627]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sample.wav 파일로 테스트. 본 파일은 an4 training set의 첫 번째 data이다.\n",
    "audio_conf = {}\n",
    "audio_conf[\"sample_rate\"] = torchaudio.load(\"./sample2.wav\")[1] #torchaudio의 2번째 return value는 해당 wav 파일의 rate이다.\n",
    "audio_conf[\"window_size\"] = 0.02\n",
    "audio_conf[\"window_stride\"] = 0.01\n",
    "audio_conf[\"window\"] = scipy.signal.hamming\n",
    "\n",
    "#class 초기화\n",
    "parser =SpectogramParser(audio_conf)\n",
    "\n",
    "#parsing된 데이터\n",
    "parser.parse_audio(\"./sample2.wav\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "pytorch의 dataset 구현. torch의 Dataset, SpectogramParser를 상속받는다.\n",
    "또한 *parse_transcript* 함수를 보면, 해당 text 가 index로 리턴이 되어야 한다. 이 또한 Data loader에 구현이 되는 것이다.\n",
    "\n",
    "Input :\n",
    "    audio_conf - 부모 클래스 spectogramparser 초기화를 위한 인자. 자료형 : dict\n",
    "    manifest_filepath - wav, input 으로 나뉘어져 있는 데이터의 경로가 저장되어있는 csv 파일. 자료형 : str\n",
    "    labels - 우리가 관측하고 싶은 character들. 자료형 : str\n",
    "\"\"\"\n",
    "class SpectogramDataset(Dataset, SpectogramParser):\n",
    "    #초기화 함수. label을 index로 변환하기 위한 딕셔너리 형태인 labels_map을 만들어야 한다.\n",
    "    def __init__(self, audio_conf, manifest_filepath, labels):\n",
    "        with open(manifest_filepath) as f:\n",
    "            ids = f.readlines()\n",
    "        ids = [x.strip().split(',') for x in ids]\n",
    "        self.ids = ids\n",
    "        self.size = len(ids)\n",
    "        self.labels_map = dict([(labels[i], i) for i in range(len(labels))])\n",
    "        super(SpectogramDataset, self).__init__(audio_conf)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        sample = self.ids[index]\n",
    "        audio_path, transcript_path = sample[0], sample[1]\n",
    "        spect = self.parse_audio(audio_path)\n",
    "        transcript = self.parse_transcript(transcript_path)\n",
    "        return spect, transcript\n",
    "\n",
    "    def parse_transcript(self, transcript_path):\n",
    "        with open(transcript_path, 'r') as transcript_file:\n",
    "            transcript = transcript_file.read().replace('\\n', '')\n",
    "        transcript = list(filter(None, [self.labels_map.get(x) for x in list(transcript)]))\n",
    "        return transcript\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *해당 클래스 example*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# sample.wav 파일로 테스트. 본 파일은 an4 training set의 첫 번째 data이다.\n",
    "audio_conf = {}\n",
    "audio_conf[\"sample_rate\"] = torchaudio.load(\"./sample.wav\")[1] #torchaudio의 2번째 return value는 해당 wav 파일의 rate이다.\n",
    "audio_conf[\"window_size\"] = 0.02\n",
    "audio_conf[\"window_stride\"] = 0.01\n",
    "audio_conf[\"window\"] = scipy.signal.hamming\n",
    "\n",
    "#초기화\n",
    "dataset = SpectogramDataset(audio_conf,\"an4_train_manifest.csv\", \"_'ABCDEFGHIJKLMNOPQRSTUVWXYZ \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\n",
       "  19.2830  19.3306  19.0667  ...   19.8727  19.3608  18.9193\n",
       "  18.7680  18.6099  18.4839  ...   19.1430  18.4066  17.5571\n",
       "  18.0558  17.3596  17.7159  ...   17.1681  17.3474  16.8489\n",
       "            ...               ⋱              ...            \n",
       "  15.5034  15.6864  16.3356  ...   14.3198  15.6247  15.4818\n",
       "  12.8209  16.2215  16.3916  ...   15.3400  15.7537  14.3083\n",
       "  16.0653  10.6976  15.2672  ...   14.5831  12.9391  15.2190\n",
       " [torch.FloatTensor of size 161x71], [9, 6, 13, 17])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 지금 slice 가 먹지를 않는다. parse audio 가 list를 인식하지 못함. 근데 굳이 slice가 먹혀야 하는가?\n",
    "dataset[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataLoader 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "collate fn 함수는 dataloader에서 minibatch를 만들어주는 default 함수이다.\n",
    "음성인식에서 minibatch를 하는 기본적인 기조는, 일단 매 음성데이터의 사이즈가 다르므로, 가장 큰 음성인식 파일에 다른 파일들의 크기를 맞추는 것이다.\n",
    "input :\n",
    "    batch - List of dataset. dataset에서 샘플링한 것의 리스트이다.\n",
    "output :\n",
    "    inputs - 4 차원의 input. N x C X H * W. 따라서 본 deep speech에서는 C는 항상 1이다.\n",
    "    targets - 1 차원의 label을 가진 IntTensor. 4번째 hidden layer를 지나면서 모든 minibatch는 concatenate된다. 따라서 1차원이다.\n",
    "    input_percentages - 1차원의 각 sample의 가장 큰 sample에 비한 상대적인 크기. 즉, 얼마만큼의 0이 삽입되었는가를 알려준다. CTC loss를 계산하는데 사용.\n",
    "    target_sizes - 각 글자 수.\n",
    "\"\"\"\n",
    "def _collate_fn(batch):\n",
    "    def func(p):\n",
    "        return p[0].size(1) # 길이.\n",
    "\n",
    "    longest_sample = max(batch, key=func)[0]\n",
    "    freq_size = longest_sample.size(0)\n",
    "    minibatch_size = len(batch)\n",
    "    max_seqlength = longest_sample.size(1) #가장 긴 sample의 길이\n",
    "    inputs = torch.zeros(minibatch_size, 1, freq_size, max_seqlength) # N X C X H X W\n",
    "    input_percentages = torch.FloatTensor(minibatch_size) \n",
    "    target_sizes = torch.IntTensor(minibatch_size)\n",
    "    targets = []\n",
    "    for x in range(minibatch_size):\n",
    "        sample = batch[x]\n",
    "        tensor = sample[0]\n",
    "        target = sample[1]\n",
    "        seq_length = tensor.size(1)\n",
    "        inputs[x][0].narrow(1, 0, seq_length).copy_(tensor)\n",
    "        input_percentages[x] = seq_length / float(max_seqlength)\n",
    "        target_sizes[x] = len(target)\n",
    "        targets.extend(target)\n",
    "    targets = torch.IntTensor(targets)\n",
    "    return inputs, targets, input_percentages, target_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DataLoader.\n",
    "enumerate로 돌아야 하며, i, (collate_fn output) 으로 iteration이 돈다.\n",
    "초기화 input: \n",
    "    dataset : spectogramdataset이 될 것\n",
    "    minibatch size : default might be 20\n",
    "    worker : loading을 위한 process의 갯수.\n",
    "\"\"\"\n",
    "class AudioDataLoader(DataLoader):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(AudioDataLoader, self).__init__(*args, **kwargs)\n",
    "        self.collate_fn = _collate_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loader = AudioDataLoader(dataset,2,4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. modeling\n",
    "\n",
    "모델의 자세한 프로세스는 다음의 그림을 참고한다 <br>\n",
    "[딥 스피치 모델](https://github.com/YBIGTA/Deep_learning/blob/master/RNN/deep_speech/implementation/%EB%AA%A8%EB%8D%B8_%EB%94%A5%EC%8A%A4%ED%94%BC%EC%B9%98.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import math\n",
    "from collections import OrderedDict\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-8feab9035334>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0m그\u001b[0m \u001b[0m필요성은\u001b[0m \u001b[0m위의\u001b[0m \u001b[0m딥\u001b[0m \u001b[0m스피치\u001b[0m \u001b[0m모델\u001b[0m \u001b[0m그림에서\u001b[0m \u001b[0m데이터가\u001b[0m \u001b[0mtranspose됨을\u001b[0m \u001b[0m보면\u001b[0m \u001b[0m알\u001b[0m \u001b[0m수\u001b[0m \u001b[0m있다\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \"\"\"\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mSequenceWise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSequenceWise\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Collapses input of dim T*N*H to (T*N)*H, and applies to a module.\n",
    "Allows handling of variable sequence lengths and minibatch sizes.\n",
    ":param module: Module to apply input to.\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "총 두 번 사용된다. CNN의 output이 RNN으로 들어갈 때. RNN의 output이 FC로 들어갈 때. \n",
    "그 필요성은 위의 딥 스피치 모델 그림에서 데이터가 transpose됨을 보면 알 수 있다.\n",
    "\"\"\"\n",
    "class SequenceWise(nn.Module):\n",
    "    def __init__(self, module):\n",
    "        super(SequenceWise, self).__init__()\n",
    "        self.module = module\n",
    "\n",
    "    def forward(self, x):\n",
    "        t, n = x.size(0), x.size(1)\n",
    "        x = x.view(t * n, -1)\n",
    "        x = self.module(x)\n",
    "        x = x.view(t, n, -1)\n",
    "        return x\n",
    "\n",
    "\"\"\"\n",
    "이건 굉장히 typical 한 minibatch softmax.\n",
    "각 data의 softmax를 torch.stack을 해야한다는 부분이 했심.\n",
    "QQQQQQQQQQQ 왜 if not self.training인가. \n",
    "\"\"\"\n",
    "class InferenceBatchSoftmax(nn.Module):\n",
    "    def forward(self, input_):\n",
    "        if not self.training:\n",
    "            batch_size = input_.size()[0]\n",
    "            return torch.stack([F.log_softmax(input_[i]) for i in range(batch_size)], 0)\n",
    "        else:\n",
    "            return input_\n",
    "        \n",
    "#class BatchSoftmax(nn.Module):\n",
    "#    def forward(self, input_):\n",
    "#        return torch.stack([F.log_softmax(input_[i] for i in range(batch_size))], 0)\n",
    "        \n",
    "\"\"\"\n",
    "batchnormalization + RNN 구현.\n",
    "굳이 이런식으로 붙이는 이유는 sequencewise 때문에. \n",
    "input : \n",
    "    input_size - T * N * H 에서 H 의 값. 여기서 T 는 time, N 은 mini batch 갯수, H는 P * C(convolution layers)\n",
    "    hidden_size - output의 값. \n",
    "                  여기서 output은 input과 같지 않냐! 고 생각하는데, 단순히 hi = W1 * xi + W2 * xi 라고 했을 때 matrix의 행의 크기. \n",
    "                  즉, T * N * H에서 H가 얼마나 압축될 것이냐의 의미.\n",
    "\"\"\"\n",
    "class BatchRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, rnn_type=nn.RNN, bidirectional=True, batch_norm=True):\n",
    "        super(BatchRNN, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.bidirectional = bidirectional\n",
    "        self.batch_norm = SequenceWise(nn.BatchNorm1d(input_size)) if batch_norm else None\n",
    "        self.rnn = rnn_type(input_size=input_size, hidden_size=hidden_size,\n",
    "                            bidirectional=bidirectional, bias=False)\n",
    "        self.num_directions = 2 if bidirectional else 1 # why do I need this?\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.batch_norm is not None:\n",
    "            x = self.batch_norm(x)\n",
    "        x, _ = self.rnn(x)\n",
    "        if self.bidirectional:\n",
    "            x = x.view(x.size(0), x.size(1), 2, -1).sum(2).view(x.size(0), x.size(1), -1)  # (TxNxH*2) -> (TxNxH) by sum\n",
    "            return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "deep speech model. \n",
    "Input : \n",
    "    없어도 된다!\n",
    "\"\"\"\n",
    "class DeepSpeech(nn.Module):\n",
    "    def __init__(self, rnn_type=nn.RNN, labels=\"_'ABCDEFGHIJKLMNOPQRSTUVWXYZ \", rnn_hidden_size=768, audio_conf=None,\n",
    "                 bidirectional=True):\n",
    "        super(DeepSpeech, self).__init__()\n",
    "\n",
    "        # model metadata needed for serialization/deserialization\n",
    "        if audio_conf is None:\n",
    "            audio_conf = {}\n",
    "        self._hidden_size = rnn_hidden_size\n",
    "        self._hidden_layers = nb_layers\n",
    "        self._rnn_type = rnn_type\n",
    "        self._audio_conf = audio_conf or {}\n",
    "        self._labels = labels\n",
    "\n",
    "        sample_rate = self._audio_conf.get(\"sample_rate\", 16000)\n",
    "        window_size = self._audio_conf.get(\"window_size\", 0.02)\n",
    "        num_classes = len(self._labels)\n",
    "\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=(41, 11), stride=(2, 2)),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.Hardtanh(0, 20, inplace=True),\n",
    "            nn.Conv2d(32, 32, kernel_size=(21, 11), stride=(2, 1)),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.Hardtanh(0, 20, inplace=True)\n",
    "        )\n",
    "        # Based on above convolutions and spectrogram size using conv formula (W - F + 2P)/ S+1\n",
    "        rnn_input_size = int(math.floor((sample_rate * window_size) / 2) + 1)\n",
    "        rnn_input_size = int(math.floor(rnn_input_size - 41) / 2 + 1)\n",
    "        rnn_input_size = int(math.floor(rnn_input_size - 21) / 2 + 1)\n",
    "        rnn_input_size *= 32\n",
    "\n",
    "        rnns = []\n",
    "        self.rnn = BatchRNN(input_size=rnn_input_size, hidden_size=rnn_hidden_size, rnn_type=rnn_type,\n",
    "                       bidirectional=bidirectional, batch_norm=True)\n",
    "\n",
    "        fully_connected = nn.Sequential(\n",
    "            nn.BatchNorm1d(rnn_hidden_size),\n",
    "            nn.Linear(rnn_hidden_size, num_classes, bias=False)\n",
    "        )\n",
    "        self.fc = nn.Sequential(\n",
    "            SequenceWise(fully_connected),\n",
    "        )\n",
    "        self.softmax = InferenceBatchSoftmax()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.coLnv(x)\n",
    "\n",
    "        sizes = x.size()\n",
    "        x = x.view(sizes[0], sizes[1] * sizes[2], sizes[3])  # Collapse feature dimension\n",
    "        x = x.transpose(1, 2).transpose(0, 1).contiguous()  # TxNxH\n",
    "\n",
    "        x = self.rnn(x)\n",
    "\n",
    "        x = self.fc(x)\n",
    "        x = x.transpose(0, 1)\n",
    "        x = self.softmax(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss function 정의.\n",
    "\n",
    "CTC loss를 정의하여야 한다. <br>\n",
    "[3 페이지 참조](https://github.com/YBIGTA/Deep_learning/blob/master/RNN/deep_speech/%EC%84%A4%EB%AA%85/Deep%20speech_%EC%83%81%ED%97%8C.pdf)\n",
    "\n",
    "현재 CTC loss는 pytorch에서 제공을 하고있지 않다. <br>\n",
    "\n",
    "따라서 [torch wrapper file](https://github.com/baidu-research/warp-ctc/blob/master/torch_binding/binding.cpp)을 [pytorch 로 transpile](https://github.com/pytorch/extension-ffi) 해야한다. <br>"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:torch]",
   "language": "python",
   "name": "conda-env-torch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
