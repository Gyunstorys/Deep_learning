{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [pytorch C extension](http://pytorch.org/tutorials/advanced/c_extension.html)\n",
    "\n",
    "**작성자 : 이상헌 ** <br>\n",
    "\n",
    "pytorch는 그 사용자 수가 tensorflow에 비하여 절대적으로 작기 때문에 다양한 model, loss가 구현이 되어있지 않다. <br>\n",
    "\n",
    "또한 torch로 구현하는 것이 속도에 큰 장점이 있기에, torch로 작성된 코드를 pytorch로 extension하는 방법이 존재한다. <br>\n",
    "(이를 wrapper라 부른다.)\n",
    "\n",
    "따라서 본 wrapper를 소개하고, 튜토리얼을 통해 그 사용법을 숙달한다. <br>\n",
    "\n",
    "### stack : <br>\n",
    "* ubuntu 16.04 <br>\n",
    "* torch 7 <br>\n",
    "\n",
    "### 0. install torch\n",
    "\n",
    "본 [torch](http://torch.ch/docs/getting-started.html#_) 링크를 따라 torch를 설치한다.. <br>\n",
    "\n",
    "### 1. Pytorch C FFI 예제 시작.\n",
    "\n",
    "본 예제는 pytorch 공식 github에서 제공하는 예제를 따라, torch로 짜여진 C code를 pytorch로 확장하는 방식을 다룬다. <br>\n",
    "\n",
    "본 예제를 통하여 extension하는 방식을 알아간다. <br>\n",
    "\n",
    "### 1-1 installation\n",
    "\n",
    "가장 본 코드를 다운받는다. <br>\n",
    "```bash\n",
    "git clone https://github.com/pytorch/extension-ffi\n",
    "```\n",
    "본 repository는 두 개의 example을 가지고 있다. <br>\n",
    "\n",
    "우리는 ./script 폴더의 예제로 실험을 진행한다. <br>\n",
    "\n",
    "script 파일은 다음과 같이 이루어져 있다. <br>\n",
    "```bashscript\n",
    "├── build.py\n",
    "├── functions\n",
    "│   ├── add.py\n",
    "│   └── __init__.py\n",
    "├── modules\n",
    "│   ├── add.py\n",
    "│   └── __init__.py\n",
    "├── README.md\n",
    "├── src\n",
    "│   ├── my_lib.c\n",
    "│   ├── my_lib_cuda.c\n",
    "│   ├── my_lib_cuda.h\n",
    "│   └── my_lib.h\n",
    "└── test.py\n",
    "```\n",
    "\n",
    "본 프로젝트에서 build.py는 torch로 작성된 모듈(my_lib.c)을 컴파일 해주는 코드이고, <br>\n",
    "test.py 는 pytorch로 본 모듈을 사용해보는 예시이다. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generating /tmp/tmpb8l3o36b/_my_lib.c\n",
      "running build_ext\n",
      "building '_my_lib' extension\n",
      "creating home\n",
      "creating home/mappiness\n",
      "creating home/mappiness/Desktop\n",
      "creating home/mappiness/Desktop/deep_learning\n",
      "creating home/mappiness/Desktop/deep_learning/Deep_learning\n",
      "creating home/mappiness/Desktop/deep_learning/Deep_learning/RNN\n",
      "creating home/mappiness/Desktop/deep_learning/Deep_learning/RNN/deep_speech\n",
      "creating home/mappiness/Desktop/deep_learning/Deep_learning/RNN/deep_speech/implementation\n",
      "creating home/mappiness/Desktop/deep_learning/Deep_learning/RNN/deep_speech/implementation/extension-ffi\n",
      "creating home/mappiness/Desktop/deep_learning/Deep_learning/RNN/deep_speech/implementation/extension-ffi/script\n",
      "creating home/mappiness/Desktop/deep_learning/Deep_learning/RNN/deep_speech/implementation/extension-ffi/script/src\n",
      "gcc -pthread -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -fPIC -I/home/mappiness/anaconda3/envs/torch/lib/python3.6/site-packages/torch/utils/ffi/../../lib/include -I/home/mappiness/anaconda3/envs/torch/lib/python3.6/site-packages/torch/utils/ffi/../../lib/include/TH -I/home/mappiness/anaconda3/envs/torch/include/python3.6m -c _my_lib.c -o ./_my_lib.o\n",
      "gcc -pthread -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -fPIC -I/home/mappiness/anaconda3/envs/torch/lib/python3.6/site-packages/torch/utils/ffi/../../lib/include -I/home/mappiness/anaconda3/envs/torch/lib/python3.6/site-packages/torch/utils/ffi/../../lib/include/TH -I/home/mappiness/anaconda3/envs/torch/include/python3.6m -c /home/mappiness/Desktop/deep_learning/Deep_learning/RNN/deep_speech/implementation/extension-ffi/script/src/my_lib.c -o ./home/mappiness/Desktop/deep_learning/Deep_learning/RNN/deep_speech/implementation/extension-ffi/script/src/my_lib.o\n",
      "gcc -pthread -shared -L/home/mappiness/anaconda3/envs/torch/lib -Wl,-rpath=/home/mappiness/anaconda3/envs/torch/lib,--no-as-needed ./_my_lib.o ./home/mappiness/Desktop/deep_learning/Deep_learning/RNN/deep_speech/implementation/extension-ffi/script/src/my_lib.o -L/home/mappiness/anaconda3/envs/torch/lib -lpython3.6m -o ./_my_lib.so\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "build.py 의 코드.\n",
    "FFI = foreign function interface. 다른 language에서 작성된 function 등을 본 language에서 사용하는 바식. \n",
    "C FFI (C 언어를 본 pytorch에서 사용할 수 있도록하는 바인딩) 를 pytorch는 제공한다.\n",
    "torch.utils.ffi 이다.\n",
    "\"\"\"\n",
    "import os\n",
    "import torch\n",
    "from torch.utils.ffi import create_extension\n",
    "\n",
    "\n",
    "# 본 프로젝트 폴더의 위치를 가져온다.\n",
    "__file__ = '/home/mappiness/Desktop/deep_learning/Deep_learning/RNN/deep_speech/implementation/extension-ffi/script/'\n",
    "\n",
    "this_file = os.path.dirname(__file__)\n",
    "\n",
    "sources = ['src/my_lib.c']\n",
    "headers = ['src/my_lib.h']\n",
    "defines = []\n",
    "with_cuda = False\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print('Including CUDA code.')\n",
    "    sources += ['src/my_lib_cuda.c']\n",
    "    headers += ['src/my_lib_cuda.h']\n",
    "    defines += [('WITH_CUDA', None)]\n",
    "    with_cuda = True\n",
    "\n",
    "ffi = create_extension(\n",
    "    '_ext.my_lib',\n",
    "    headers=headers,\n",
    "    sources=sources,\n",
    "    define_macros=defines,\n",
    "    relative_to=__file__,\n",
    "    with_cuda=with_cuda\n",
    ")\n",
    "\n",
    "# FFI는 build의 단계에서 컴파일러가 코드를 해석할 수 있도록 만들어준다. 따라서 함수 이름이 build인 것이다.\n",
    "ffi.build()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "본 FFI를 build하면 다음과 같은 파일들이 생성된다. <br>\n",
    "```bash\n",
    "./_ext/\n",
    "├── __init__.py\n",
    "└── my_lib\n",
    "    ├── __init__.py\n",
    "    └── _my_lib.so\n",
    "```\n",
    "\n",
    "이제 본 my_lib을 import하여 사용하면 되는 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*my_lib.c 코드. 본 코드는 두 node를 더하는 함수의 forward와 backward를 구현해 놓은 함수이다.*\n",
    "\n",
    "```C\n",
    "#include <TH/TH.h>\n",
    "\n",
    "int my_lib_add_forward(THFloatTensor *input1, THFloatTensor *input2,\n",
    "\t\t       THFloatTensor *output)\n",
    "{\n",
    "  if (!THFloatTensor_isSameSizeAs(input1, input2))\n",
    "    return 0;\n",
    "  THFloatTensor_resizeAs(output, input1);\n",
    "  THFloatTensor_cadd(output, input1, 1.0, input2);\n",
    "  return 1;\n",
    "}\n",
    "\n",
    "int my_lib_add_backward(THFloatTensor *grad_output, THFloatTensor *grad_input)\n",
    "{\n",
    "  THFloatTensor_resizeAs(grad_input, grad_output);\n",
    "  THFloatTensor_fill(grad_input, 1);\n",
    "  return 1;\n",
    "}\n",
    "```\n",
    "\n",
    "1. 이제 단순히 torch.autograd.Function에서 본 함수를 wrapping 해주고, <br>\n",
    "2. 그 wrapping된 function을 torch.nn.Module을 통해 wrapping 해주면 사용할 수 있다. <br>\n",
    "\n",
    "여기서 핵심은 function class의 경우에는 forward와 backward가 있지만, <br>\n",
    "module function은 forward만 정의해주면 된다는 것!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### functions/add.py\n",
    "\n",
    "``` python\n",
    "import torch\n",
    "from torch.autograd import Function\n",
    "from _ext import my_lib\n",
    "\n",
    "\n",
    "class MyAddFunction(Function):\n",
    "    def forward(self, input1, input2):\n",
    "        output = input1.new()\n",
    "        if not input1.is_cuda:\n",
    "            my_lib.my_lib_add_forward(input1, input2, output)\n",
    "        else:\n",
    "            my_lib.my_lib_add_forward_cuda(input1, input2, output)\n",
    "        return output\n",
    "\n",
    "    def backward(self, grad_output):\n",
    "        grad_input = grad_output.new()\n",
    "        if not grad_output.is_cuda:\n",
    "            my_lib.my_lib_add_backward(grad_output, grad_input)\n",
    "        else:\n",
    "            my_lib.my_lib_add_backward_cuda(grad_output, grad_input)\n",
    "        return grad_input\n",
    "```\n",
    "\n",
    "### modules/add.py\n",
    "\n",
    "```python\n",
    "from torch.nn.modules.module import Module\n",
    "from functions.add import MyAddFunction\n",
    "\n",
    "class MyAddModule(Module):\n",
    "    def forward(self, input1, input2):\n",
    "        return MyAddFunction()(input1, input2) # 이 형태가 매우 불안정해.\n",
    "```\n",
    "[참고](https://github.com/pytorch/pytorch/issues/821)\n",
    "\n",
    "### cf) MyAddFunction()(input1, input2) 이란?\n",
    "\n",
    "function을 보다시피 autograd, 즉 gradient를 저장해놔야 한다. <br>\n",
    "gradient의 경우 static 변수로 선언이 되어있는데, <br>\n",
    "이렇게 선언이 되어있으므로 forward의 경우엔 static function으로 callable 하다. <br>\n",
    "그 function이 단순히 Myfunction()(\\*\\*kargs) 로 구현이 되어있는 것이다. <br>\n",
    "반드시 autograd.function은 인스턴스로 만들지 않고 콜링을 해야하며, <br>\n",
    "forward는 초기화와 함께 인자를 넘기는 방식으로만 사용한다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:torch]",
   "language": "python",
   "name": "conda-env-torch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
