{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DEEP SPEECH1 구현 - pytorch\n",
    "\n",
    "CTC loss 를 이용한 deep speech 구현이다. <br>\n",
    "구현 모델은 [Deep speech 이론 by YBigTa](https://github.com/YBIGTA/Deep_learning/blob/master/RNN/deep%20speech/%EC%84%A4%EB%AA%85/Deep%20speech_%EC%83%81%ED%97%8C.pdf) 에서 확인 가능하다. <br>\n",
    "\n",
    "본 코드는 [deep speech2 implementation](https://github.com/SeanNaren/deepspeech.pytorch/)을 상당부분 참고한다.\n",
    "\n",
    "### 구현 stack\n",
    "OS : ubuntu 16.04 <br>\n",
    "conda : 4.2.9 <br>\n",
    "\n",
    "\n",
    "## 설치\n",
    "\n",
    "오디오 I/O 를 위한 pytorch audio를 설치한다. <br>\n",
    "```\n",
    "sudo apt-get install sox libsox-dev libsox-fmt-all\n",
    "git clone https://github.com/pytorch/audio.git\n",
    "cd audio\n",
    "python setup.py install\n",
    "```\n",
    "\n",
    "Requirments를 설치한다. <br>\n",
    "```\n",
    "sudo pip install python-levenshtein torch visdom wget librosa\n",
    "```\n",
    "\n",
    "## 0. 데이터셋 다운로드\n",
    "\n",
    "데이터셋의 경우 카네기 멜론 대학교에서 제공한 free dataset인 **AN4** 를 사용한다.\n",
    "\n",
    "```python\n",
    "import argparse\n",
    "import os\n",
    "import io\n",
    "import shutil\n",
    "import tarfile\n",
    "import wget\n",
    "\n",
    "import subprocess\n",
    "\n",
    "def _order_files(file_paths):\n",
    "    print(\"Sorting files by length...\")\n",
    "\n",
    "    def func(element):\n",
    "        output = subprocess.check_output(\n",
    "            ['soxi -D \\\"%s\\\"' % element.strip()],\n",
    "            shell=True\n",
    "        )\n",
    "        return float(output)\n",
    "\n",
    "    file_paths.sort(key=func)\n",
    "    \n",
    "def create_manifest(data_path, tag, ordered=True):\n",
    "    manifest_path = '%s_manifest.csv' % tag\n",
    "    file_paths = []\n",
    "    wav_files = [os.path.join(dirpath, f)\n",
    "                 for dirpath, dirnames, files in os.walk(data_path)\n",
    "                 for f in fnmatch.filter(files, '*.wav')]\n",
    "    size = len(wav_files)\n",
    "    counter = 0\n",
    "    for file_path in wav_files:\n",
    "        file_paths.append(file_path.strip())\n",
    "        counter += 1\n",
    "    print('\\n')\n",
    "    if ordered:\n",
    "        _order_files(file_paths)\n",
    "    counter = 0\n",
    "    with io.FileIO(manifest_path, \"w\") as file:\n",
    "        for wav_path in file_paths:\n",
    "            transcript_path = wav_path.replace('/wav/', '/txt/').replace('.wav', '.txt')\n",
    "            sample = os.path.abspath(wav_path) + ',' + os.path.abspath(transcript_path) + '\\n'\n",
    "            file.write(sample.encode('utf-8'))\n",
    "            counter += 1\n",
    "    print('\\n')\n",
    "\n",
    "# command line에서 동작하는 것들을 더 쉽게 만들어주는 parser를 이용하여 데이터를 다운받는다.\n",
    "parser = argparse.ArgumentParser(description='Processes and downloads an4.') \n",
    "parser.add_argument('--target_dir', default='an4_dataset/', help='Path to save dataset')\n",
    "parser.add_argument('--sample_rate', default=16000, type=int, help='Sample rate')\n",
    "args = parser.parse_args()\n",
    "\n",
    "\n",
    "def _format_data(root_path, data_tag, name, wav_folder):\n",
    "    data_path = args.target_dir + data_tag + '/' + name + '/'\n",
    "    new_transcript_path = data_path + '/txt/'\n",
    "    new_wav_path = data_path + '/wav/'\n",
    "\n",
    "    os.makedirs(new_transcript_path)\n",
    "    os.makedirs(new_wav_path)\n",
    "\n",
    "    wav_path = root_path + 'wav/'\n",
    "    file_ids = root_path + 'etc/an4_%s.fileids' % data_tag\n",
    "    transcripts = root_path + 'etc/an4_%s.transcription' % data_tag\n",
    "    train_path = wav_path + wav_folder\n",
    "\n",
    "    _convert_audio_to_wav(train_path)\n",
    "    _format_files(file_ids, new_transcript_path, new_wav_path, transcripts, wav_path)\n",
    "\n",
    "\n",
    "def _convert_audio_to_wav(train_path):\n",
    "    with os.popen('find %s -type f -name \"*.raw\"' % train_path) as pipe:\n",
    "        for line in pipe:\n",
    "            raw_path = line.strip()\n",
    "            new_path = line.replace('.raw', '.wav').strip()\n",
    "            cmd = 'sox -t raw -r %d -b 16 -e signed-integer -B -c 1 \\\"%s\\\" \\\"%s\\\"' % (\n",
    "                args.sample_rate, raw_path, new_path)\n",
    "            os.system(cmd)\n",
    "\n",
    "\n",
    "def _format_files(file_ids, new_transcript_path, new_wav_path, transcripts, wav_path):\n",
    "    with open(file_ids, 'r') as f:\n",
    "        with open(transcripts, 'r') as t:\n",
    "            paths = f.readlines()\n",
    "            transcripts = t.readlines()\n",
    "            for x in range(len(paths)):\n",
    "                path = wav_path + paths[x].strip() + '.wav'\n",
    "                filename = path.split('/')[-1]\n",
    "                extracted_transcript = _process_transcript(transcripts, x)\n",
    "                current_path = os.path.abspath(path)\n",
    "                new_path = new_wav_path + filename\n",
    "                text_path = new_transcript_path + filename.replace('.wav', '.txt')\n",
    "                with io.FileIO(text_path, \"w\") as file:\n",
    "                    file.write(extracted_transcript.encode('utf-8'))\n",
    "                os.rename(current_path, new_path)\n",
    "\n",
    "\n",
    "def _process_transcript(transcripts, x):\n",
    "    extracted_transcript = transcripts[x].split('(')[0].strip(\"<s>\").split('<')[0].strip().upper()\n",
    "    return extracted_transcript\n",
    "\n",
    "\n",
    "def main():\n",
    "    root_path = 'an4/'\n",
    "    name = 'an4'\n",
    "    wget.download('http://www.speech.cs.cmu.edu/databases/an4/an4_raw.bigendian.tar.gz')\n",
    "    tar = tarfile.open('an4_raw.bigendian.tar.gz')\n",
    "    tar.extractall()\n",
    "    os.makedirs(args.target_dir)\n",
    "    _format_data(root_path, 'train', name, 'an4_clstk')\n",
    "    _format_data(root_path, 'test', name, 'an4test_clstk')\n",
    "    shutil.rmtree(root_path)\n",
    "    os.remove('an4_raw.bigendian.tar.gz')\n",
    "    train_path = args.target_dir + '/train/'\n",
    "    test_path = args.target_dir + '/test/'\n",
    "    print ('\\n', 'Creating manifests...')\n",
    "    create_manifest(train_path, 'an4_train')\n",
    "    create_manifest(test_path, 'an4_val')\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Making audio spectogram using fourier transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset label csv 파일 제작\n",
    "\n",
    "### training, testing을 위한 data는 다음과 같은 형태로 폴더 내에 저장이 되어 있다. \n",
    "\n",
    "#### training \n",
    "\n",
    "*data/data_path_to_train/input* <br>\n",
    "*data/data_path_to_train/label* <br>\n",
    "\n",
    "#### testing\n",
    "\n",
    "*data/data_path_to_test/input* <br>\n",
    "*data/data_path_to_test/label* <br>\n",
    "\n",
    "따라서 본 데이터를 load 하기 위하여 training 시, testing 시 각각 불러올 파일들의 path를 하나의 csv파일(manifest라 부른다.) 로 저장하면 부르기에 용이하다. <br>\n",
    "\n",
    "위의 코드 중 create_manifest 함수가 본 파일을 csv화 해주는 함수이다. <br>\n",
    "\n",
    "따라서 코드를 돌린 장소에 csv 파일을 참조한다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset과 Dataloader 구현\n",
    "\n",
    "pytorch의 dataset과 dataloader의 경우는 그 클래스가 정해져 있다. <br>\n",
    "\n",
    "따라서 data loader와 dataset의 경우에는 사용자 정의 클래스를 만들기 위해서 **torch.utils.data**의 **Dataset**과 **DataLoader**를 상속받아야 한다. <br>\n",
    "\n",
    "이를 통해 사용자 정의 dataset, dataloader를 구현할 것이며, <br>\n",
    "\n",
    "본 dataset에는 audio spectogram 형태의 dataset이여야 하므로 *wav* 파일을 *spectogram* 형태로 바꾸는 함수를 집어넣고 구현한다., <br>\n",
    "\n",
    "추가적인 dataset, dataloader 구현 튜토리얼은 [pytorch 공인 튜토리얼](http://pytorch.org/tutorials/beginner/data_loading_tutorial.html) 을 참고한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 필요 class load\n",
    "import librosa\n",
    "import numpy as np\n",
    "import scipy.signal\n",
    "import torch\n",
    "import torchaudio\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "wav 파일을 load하는 함수.\n",
    "torchaudio를 사용한다. \n",
    "\n",
    "Input : \n",
    "    wav 파일 path. 자료형 : str\n",
    "Output :\n",
    "    오디오 파일의 numpy 형태. 자료형 : np\n",
    "\"\"\"\n",
    "def load_audio(path):\n",
    "    sound, _ = torchaudio.load(path)\n",
    "    sound = sound.numpy()\n",
    "    if len(sound.shape) > 1:\n",
    "        if sound.shape[1] == 1:\n",
    "            sound = sound.squeeze()\n",
    "        else:\n",
    "            sound = sound.mean(axis=1)  # multiple channels, average\n",
    "    return sound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Fourier transformation parser.\n",
    "spectogramparser는 spectogramDataset에 상속되며, 모든 wav 파일을 spectogram으로 파싱하는 클래스이다.\n",
    "\n",
    "Param : \n",
    "    audio_conf - 오디오 특성의 딕셔너리. spectogram화를 위한 window 방식, window size, stride, 음성의 rate 가 들어있어야 한다. 자료형 : dict\n",
    "\"\"\"\n",
    "class SpectogramParser(object):\n",
    "    # 초기화 함수. \n",
    "    def __init__(self, audio_conf):\n",
    "        super(SpectogramParser, self).__init__()\n",
    "        self.window_stride = audio_conf['window_stride']\n",
    "        self.window_size = audio_conf['window_size']\n",
    "        self.sample_rate = audio_conf['sample_rate']\n",
    "        self.window = audio_conf['window']\n",
    "    \n",
    "    \"\"\"\n",
    "    parsing 함수. \n",
    "    Input : \n",
    "        audio_path. 자료형 : dict\n",
    "    output : \n",
    "        해당 path의 spectogram, 자료형 : FloatTensor (MHz + 1, len)\n",
    "    \"\"\"\n",
    "    def parse_audio(self, audio_path):\n",
    "        \n",
    "        y = load_audio(audio_path)\n",
    "        n_fft = int(self.sample_rate * self.window_size)\n",
    "        win_length = n_fft\n",
    "        hop_length = int(self.sample_rate * self.window_stride)\n",
    "        # STFT\n",
    "        D = librosa.stft(y, n_fft=n_fft, hop_length=hop_length,\n",
    "                         win_length=win_length, window=self.window)\n",
    "        spect, phase = librosa.magphase(D)\n",
    "        # S = log(S+1)\n",
    "        spect = np.log1p(spect)\n",
    "        spect = torch.FloatTensor(spect)\n",
    "        return spect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *해당 클래스 example*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 18.7441  17.1461  17.6008  ...   18.2615  11.8843  17.3191\n",
       " 17.9347  18.3567  18.4088  ...   18.1678  18.2587  15.9493\n",
       " 17.3988  17.9348  18.1413  ...   18.9098  17.7493  17.6598\n",
       "           ...               ⋱              ...            \n",
       " 15.4377  17.1619  15.5005  ...   15.3234  15.6830  15.5769\n",
       " 16.3273  16.5899  14.5669  ...   15.5692  15.3346  14.6444\n",
       " 16.0336  16.2428  15.1358  ...   15.8261  14.1488  13.8953\n",
       "[torch.FloatTensor of size 442x55]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sample.wav 파일로 테스트. 본 파일은 an4 training set의 첫 번째 data이다.\n",
    "audio_conf = {}\n",
    "audio_conf[\"sample_rate\"] = torchaudio.load(\"./sample.wav\")[1] #torchaudio의 2번째 return value는 해당 wav 파일의 rate이다.\n",
    "audio_conf[\"window_size\"] = 0.02\n",
    "audio_conf[\"window_stride\"] = 0.01\n",
    "audio_conf[\"window\"] = scipy.signal.hamming\n",
    "\n",
    "#class 초기화\n",
    "parser =SpectogramParser(audio_conf)\n",
    "\n",
    "#parsing된 데이터\n",
    "parser.parse_audio(\"./sample.wav\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loader 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "pytorch의 dataset 구현. torch의 Dataset, SpectogramParser를 상속받는다.\n",
    "또한 *parse_transcript* 함수를 보면, 해당 text 가 index로 리턴이 되어야 한다. 이 또한 Data loader에 구현이 되는 것이다.\n",
    "\n",
    "Input :\n",
    "    audio_conf - 부모 클래스 spectogramparser 초기화를 위한 인자. 자료형 : dict\n",
    "    manifest_filepath - wav, input 으로 나뉘어져 있는 데이터의 경로가 저장되어있는 csv 파일. 자료형 : str\n",
    "    labels - 우리가 관측하고 싶은 character들. 자료형 : str\n",
    "\"\"\"\n",
    "class SpectogramDataset(Dataset, SpectogramParser):\n",
    "    #초기화 함수. label을 index로 변환하기 위한 딕셔너리 형태인 labels_map을 만들어야 한다.\n",
    "    def __init__(self, audio_conf, manifest_filepath, labels):\n",
    "        with open(manifest_filepath) as f:\n",
    "            ids = f.readlines()\n",
    "        ids = [x.strip().split(',') for x in ids]\n",
    "        self.ids = ids\n",
    "        self.size = len(ids)\n",
    "        self.labels_map = dict([(labels[i], i) for i in range(len(labels))])\n",
    "        super(SpectogramDataset, self).__init__(audio_conf)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        sample = self.ids[index]\n",
    "        audio_path, transcript_path = sample[0], sample[1]\n",
    "        spect = self.parse_audio(audio_path)\n",
    "        transcript = self.parse_transcript(transcript_path)\n",
    "        return spect, transcript\n",
    "\n",
    "    def parse_transcript(self, transcript_path):\n",
    "        with open(transcript_path, 'r') as transcript_file:\n",
    "            transcript = transcript_file.read().replace('\\n', '')\n",
    "        transcript = list(filter(None, [self.labels_map.get(x) for x in list(transcript)]))\n",
    "        return transcript\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(\"an4_train_manifest.csv\") as f:\n",
    "    ids = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ids = [x.strip().split(',') for x in ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labels = \"_'ABCDEFGHIJKLMNOPQRSTUVWXYZ \"\n",
    "labels_map = dict([(labels[i], i) for i in range(len(labels))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "parser =SpectogramParser(audio_conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "audio_path = ids[0][0]\n",
    "transcript_path = ids[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/mappiness/Desktop/deep learning/Deep_learning/RNN/deep speech/구현/an4_dataset/train/an4/wav/an253-fash-b.wav'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *해당 클래스 example*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# sample.wav 파일로 테스트. 본 파일은 an4 training set의 첫 번째 data이다.\n",
    "audio_conf = {}\n",
    "audio_conf[\"sample_rate\"] = torchaudio.load(\"./sample.wav\")[1] #torchaudio의 2번째 return value는 해당 wav 파일의 rate이다.\n",
    "audio_conf[\"window_size\"] = 0.02\n",
    "audio_conf[\"window_stride\"] = 0.01\n",
    "audio_conf[\"window\"] = scipy.signal.hamming\n",
    "\n",
    "#초기화\n",
    "dataset = SpectogramDataset(audio_conf,\"an4_train_manifest.csv\", \"_'ABCDEFGHIJKLMNOPQRSTUVWXYZ \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "UnicodeEncodeError",
     "evalue": "'ascii' codec can't encode characters in position 68-69: ordinal not in range(128)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnicodeEncodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-77e4bc312954>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-25-1a88c54546a7>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0maudio_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtranscript_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0mspect\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_audio\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maudio_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0mtranscript\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_transcript\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtranscript_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mspect\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtranscript\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-19-b97e017f3c2b>\u001b[0m in \u001b[0;36mparse_audio\u001b[0;34m(self, audio_path)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mparse_audio\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maudio_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_audio\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maudio_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0mn_fft\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_rate\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwindow_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mwin_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mn_fft\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-18-35e74f963dd4>\u001b[0m in \u001b[0;36mload_audio\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m      9\u001b[0m \"\"\"\n\u001b[1;32m     10\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_audio\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0msound\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorchaudio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0msound\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msound\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msound\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/mappiness/anaconda3/envs/torch/lib/python3.6/site-packages/torchaudio-0.1-py3.6-linux-x86_64.egg/torchaudio/__init__.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(filename, out)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mth_sox\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'libthsox_{}_read_audio_file'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtypename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0msample_rate_p\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mffi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'int*'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ascii\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_rate_p\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m     \u001b[0msample_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample_rate_p\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_rate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnicodeEncodeError\u001b[0m: 'ascii' codec can't encode characters in position 68-69: ordinal not in range(128)"
     ]
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. modeling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import math\n",
    "from collections import OrderedDict\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\"\"\"\n",
    "RNN 모델상에서의 사용되는 4번째 layer RNN.\n",
    "deep speech(2 아닌 1)논문에서는 bidirectional RNN을 사용한다.\n",
    "\"\"\"\n",
    "supported_rnns = {\n",
    "    'lstm': nn.LSTM,\n",
    "    'rnn': nn.RNN,\n",
    "    'gru': nn.GRU\n",
    "}\n",
    "supported_rnns_inv = dict((v, k) for k, v in supported_rnns.items()) #내가봤을 때 inverse는 딱히 필요없어."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Collapses input of dim T*N*H to (T*N)*H, and applies to a module.\n",
    "Allows handling of variable sequence lengths and minibatch sizes.\n",
    ":param module: Module to apply input to.\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "그니까 이게 minibatch를 하나의 string으로 만들어준다는 거지.\n",
    "Q.  왜 굳이 그렇게하지? \n",
    "그냥 long sentence training을 위하여?\n",
    "\"\"\"\n",
    "class SequenceWise(nn.Module):\n",
    "    def __init__(self, module):\n",
    "        super(SequenceWise, self).__init__()\n",
    "        self.module = module\n",
    "\n",
    "    def forward(self, x):\n",
    "        t, n = x.size(0), x.size(1)\n",
    "        x = x.view(t * n, -1)\n",
    "        x = self.module(x)\n",
    "        x = x.view(t, n, -1)\n",
    "        return x\n",
    "\n",
    "    def __repr__(self):\n",
    "        tmpstr = self.__class__.__name__ + ' (\\n'\n",
    "        tmpstr += self.module.__repr__()\n",
    "        tmpstr += ')'\n",
    "        return tmpstr\n",
    "    \n",
    "\"\"\"\n",
    "이건 굉장히 typical 한 minibatch softmax.\n",
    "각 data의 softmax를 torch.stack을 해야한다는거지.\n",
    "Q. 근데 여기서 self.training은 어디서 오는거야\n",
    "\"\"\"\n",
    "class InferenceBatchSoftmax(nn.Module):\n",
    "    def forward(self, input_):\n",
    "        if not self.training:\n",
    "            batch_size = input_.size()[0]\n",
    "            return torch.stack([F.log_softmax(input_[i]) for i in range(batch_size)], 0)\n",
    "        else:\n",
    "            return input_\n",
    "        \n",
    "\"\"\"\n",
    "batch구현\n",
    "왜 batch normalization data를 sequencewise를 이 단에서 구현하느냐.\n",
    "batch normalization에 대한 ref) https://shuuki4.wordpress.com/2016/01/13/batch-normalization-%EC%84%A4%EB%AA%85-%EB%B0%8F-%EA%B5%AC%ED%98%84/\n",
    "이 sequencewise를 없애고 해보자 나중에.\n",
    "\"\"\"\n",
    "class BatchRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, rnn_type=nn.LSTM, bidirectional=False, batch_norm=True):\n",
    "        super(BatchRNN, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.bidirectional = bidirectional\n",
    "        self.batch_norm = SequenceWise(nn.BatchNorm1d(input_size)) if batch_norm else None\n",
    "        self.rnn = rnn_type(input_size=input_size, hidden_size=hidden_size,\n",
    "                            bidirectional=bidirectional, bias=False)\n",
    "        self.num_directions = 2 if bidirectional else 1\n",
    "\n",
    "    # Q. 그니까 RNN 의 hidden layer를 output으로 내는데, bidirectional의 경우에는 이 둘을 그냥 더한다?\n",
    "    def forward(self, x):\n",
    "        if self.batch_norm is not None:\n",
    "            x = self.batch_norm(x)\n",
    "        x, _ = self.rnn(x)\n",
    "        if self.bidirectional:\n",
    "            x = x.view(x.size(0), x.size(1), 2, -1).sum(2).view(x.size(0), x.size(1), -1)  # (TxNxH*2) -> (TxNxH) by sum\n",
    "            return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "Q. labels의 역할은? 왜 여긴 abc밖에 없는거지?\n",
    "Q. audio_conf는?\n",
    "\"\"\"\n",
    "class DeepSpeech(nn.Module):\n",
    "    def __init__(self, rnn_type=nn.LSTM, labels=\"abc\", rnn_hidden_size=768, nb_layers=5, audio_conf=None,\n",
    "                 bidirectional=True):\n",
    "        super(DeepSpeech, self).__init__()\n",
    "\n",
    "        # model metadata needed for serialization/deserialization\n",
    "        if audio_conf is None:\n",
    "            audio_conf = {}\n",
    "        self._version = '0.0.1'\n",
    "        self._hidden_size = rnn_hidden_size\n",
    "        self._hidden_layers = nb_layers\n",
    "        self._rnn_type = rnn_type\n",
    "        self._audio_conf = audio_conf or {}\n",
    "        self._labels = labels\n",
    "\n",
    "        sample_rate = self._audio_conf.get(\"sample_rate\", 16000)\n",
    "        window_size = self._audio_conf.get(\"window_size\", 0.02)\n",
    "        num_classes = len(self._labels)\n",
    "\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=(41, 11), stride=(2, 2)),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.Hardtanh(0, 20, inplace=True),\n",
    "            nn.Conv2d(32, 32, kernel_size=(21, 11), stride=(2, 1)),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.Hardtanh(0, 20, inplace=True)\n",
    "        )\n",
    "        # Based on above convolutions and spectrogram size using conv formula (W - F + 2P)/ S+1\n",
    "        rnn_input_size = int(math.floor((sample_rate * window_size) / 2) + 1)\n",
    "        rnn_input_size = int(math.floor(rnn_input_size - 41) / 2 + 1)\n",
    "        rnn_input_size = int(math.floor(rnn_input_size - 21) / 2 + 1)\n",
    "        rnn_input_size *= 32\n",
    "\n",
    "        rnns = []\n",
    "        rnn = BatchRNN(input_size=rnn_input_size, hidden_size=rnn_hidden_size, rnn_type=rnn_type,\n",
    "                       bidirectional=bidirectional, batch_norm=False)\n",
    "        rnns.append(('0', rnn))\n",
    "        for x in range(nb_layers - 1):\n",
    "            rnn = BatchRNN(input_size=rnn_hidden_size, hidden_size=rnn_hidden_size, rnn_type=rnn_type,\n",
    "                           bidirectional=bidirectional)\n",
    "            rnns.append(('%d' % (x + 1), rnn))\n",
    "        self.rnns = nn.Sequential(OrderedDict(rnns))\n",
    "        fully_connected = nn.Sequential(\n",
    "            nn.BatchNorm1d(rnn_hidden_size),\n",
    "            nn.Linear(rnn_hidden_size, num_classes, bias=False)\n",
    "        )\n",
    "        self.fc = nn.Sequential(\n",
    "            SequenceWise(fully_connected),\n",
    "        )\n",
    "        self.softmax = InferenceBatchSoftmax()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "\n",
    "        sizes = x.size()\n",
    "        x = x.view(sizes[0], sizes[1] * sizes[2], sizes[3])  # Collapse feature dimension\n",
    "        x = x.transpose(1, 2).transpose(0, 1).contiguous()  # TxNxH\n",
    "\n",
    "        x = self.rnns(x)\n",
    "\n",
    "        x = self.fc(x)\n",
    "        x = x.transpose(0, 1)\n",
    "        x = self.softmax(x)\n",
    "        return x\n",
    "\n",
    "    @classmethod\n",
    "    def load_model(cls, path, cuda=False):\n",
    "        package = torch.load(path, map_location=lambda storage, loc: storage)\n",
    "        model = cls(rnn_hidden_size=package['hidden_size'], nb_layers=package['hidden_layers'],\n",
    "                    labels=package['labels'], audio_conf=package['audio_conf'],\n",
    "                    rnn_type=supported_rnns[package['rnn_type']])\n",
    "        model.load_state_dict(package['state_dict'])\n",
    "        if cuda:\n",
    "            model = torch.nn.DataParallel(model).cuda()\n",
    "        return model\n",
    "\n",
    "    @staticmethod\n",
    "    def serialize(model, optimizer=None, epoch=None, iteration=None, loss_results=None,\n",
    "                  cer_results=None, wer_results=None, avg_loss=None, meta=None):\n",
    "        model_is_cuda = next(model.parameters()).is_cuda\n",
    "        model = model.module if model_is_cuda else model\n",
    "        package = {\n",
    "            'version': model._version,\n",
    "            'hidden_size': model._hidden_size,\n",
    "            'hidden_layers': model._hidden_layers,\n",
    "            'rnn_type': supported_rnns_inv.get(model._rnn_type, model._rnn_type.__name__.lower()),\n",
    "            'audio_conf': model._audio_conf,\n",
    "            'labels': model._labels,\n",
    "            'state_dict': model.state_dict()\n",
    "        }\n",
    "        if optimizer is not None:\n",
    "            package['optim_dict'] = optimizer.state_dict()\n",
    "        if avg_loss is not None:\n",
    "            package['avg_loss'] = avg_loss\n",
    "        if epoch is not None:\n",
    "            package['epoch'] = epoch + 1  # increment for readability\n",
    "        if iteration is not None:\n",
    "            package['iteration'] = iteration\n",
    "        if loss_results is not None:\n",
    "            package['loss_results'] = loss_results\n",
    "            package['cer_results'] = cer_results\n",
    "            package['wer_results'] = wer_results\n",
    "        if meta is not None:\n",
    "            package['meta'] = meta\n",
    "        return package\n",
    "\n",
    "    @staticmethod\n",
    "    def get_labels(model):\n",
    "        model_is_cuda = next(model.parameters()).is_cuda\n",
    "        return model.module._labels if model_is_cuda else model._labels\n",
    "\n",
    "    @staticmethod\n",
    "    def get_param_size(model):\n",
    "        params = 0\n",
    "        for p in model.parameters():\n",
    "            tmp = 1\n",
    "            for x in p.size():\n",
    "                tmp *= x\n",
    "            params += tmp\n",
    "        return params\n",
    "\n",
    "    @staticmethod\n",
    "    def get_audio_conf(model):\n",
    "        model_is_cuda = next(model.parameters()).is_cuda\n",
    "        return model.module._audio_conf if model_is_cuda else model._audio_conf\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(\"abc\")"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:torch]",
   "language": "python",
   "name": "conda-env-torch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
