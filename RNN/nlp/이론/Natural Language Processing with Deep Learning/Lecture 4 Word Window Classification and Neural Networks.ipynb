{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 4 | Word Window Classification and Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "강의 출처 : https://www.youtube.com/watch?v=uc2_iwVqrRI&index=4&list=PL3FW7Lu3i5Jsnh1rnUwq_TcylNr7EkRe6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![rnn link](http://i.imgur.com/sAgNpi1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- W의 dim = [C,d]이고, x의 dim = [d,1]이다. (C = # of classes, d = 단어를 vector로 표현하는 길이)\n",
    "- y는 1~C 중에 속해있는 하나의 class이다. (올바른 target) \n",
    "- 각 W의 row(1~C)에 x를 곱한 값들의 합의 분모이다.\n",
    "- 1~C 중에 속해있는 하나의 class y에 해당하는 W의 row(y번째)에 x를 곱한 값이 분자이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![rnn link](http://i.imgur.com/tmstwpR.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 1번 : W의 y번째 row(dim = [1,d])와 x(dim = [d,1])을 내적한 값이다.\n",
    "- 2번 : exp(해당 내적값) / 전체 exp(내적값) = p 확률이 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![rnn link](http://i.imgur.com/99a3UAV.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "objective function = maximizing probability = maximizing log probability = minimizing the negative of that log probability "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![rnn link](http://i.imgur.com/s33gUFL.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- p = 실제 probability distribution [0,0,...,1,...,0,0]\n",
    "- q = softmax 결과\n",
    "- p가 one-hot-vector이므로, p(c) * log q(c)는 결과적으로 log q(c)만 남는다.\n",
    "- 전체 H(p,q) = true class위치의 negative log probability of softmax output\n",
    "- (p(c) = 0 or 1 single number)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![rnn link](http://i.imgur.com/DROlKUB.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위에서는 (x,y)하나의 data에 대해서 loss를 구하였다. 전체 dataset(1...N)에 대한 loss의 평균이 전체 dataset의 loss이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![rnn link](http://i.imgur.com/nWcXUtZ.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "전체 data의 loss를 구하는 과정에서, overfitting을 피하기 위해서 parameter seta를 추가해준다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![rnn link](http://i.imgur.com/aOzA7Aa.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "vector로 나타낸 'telly', 'TV', 'television' 모두 가까운 거리에 위치해있고, classification도 잘 이루어졌다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![rnn link](http://i.imgur.com/IM1K3bG.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 제한된 training set size로 인해 'telly', 'TV' 두 개의 단어만 retrain시켰다.\n",
    "- 두 개의 단어 'telly', 'TV'만 위치가 변경되었고, 이로 인해 boundary가 달라져, 'television'은 misclassify 되었다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![rnn link](http://i.imgur.com/gc5VwSp.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "결론 : small training data set만 있는 경우에는 training 시키지 말아라."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![rnn link](http://i.imgur.com/TSwtTMe.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "단순히 single word만 활용하여 classify 시키는 것은, 위와 같은 단어의 이중적인 의미(모호함) 때문에 좋은 결과를 낼 수 없다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![rnn link](http://i.imgur.com/GHNzO6x.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "따라서 따라서 주변 단어들의 문맥을 보고 해당 단어를 classify하는 것이 window classification이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![rnn link](http://i.imgur.com/kzXgCMe.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- window length = 2 이면, 해당 center word를 중심으로 앞 뒤 2개의 단어를 하나의 vector x로 본다.\n",
    "- 따라서 input으로 사용되는 vector x의 차원도 변한다. 기존의 x의 dim = [d,1] 이였다면, window length = 2인 window vector x의 dim = [5d,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이후에는 softmax함수에 대해 backpropagation의 과정을 거칠 때 사용할만한 tip을 제시해주는데, 설명은 생략하겠다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![rnn link](http://i.imgur.com/WSMbIuZ.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for 문을 사용해서 행렬의 곱셈을 하는 것 보다는, 행렬곱을 이용하는 것이 훨씬 속도가 빠르다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이후에는 logistic regression의 한계를 언급하면서 Neural Net의 도입을 알린다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![rnn link](http://i.imgur.com/uDuqDD8.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 우리의 목표는 center word에 지명에 해당하는 단어(ex-Paris)가 들어왔을 때 높은 score를 주고, 아닌 경우에는 낮은 score를 주는 것이다.\n",
    "- S : score of center word = paris = named entity location\n",
    "- Sc : score of center word = museums = not a named entity location(우리가 원하는 location이 center word가 아닌 window는 다른 class로 인식)\n",
    "- minimize J의 방향으로 training 시켜준다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "boundary region을 그리면, max-margin loss는 maximize the distance between the closest point 형태로 나타난다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 s를 variable U,W,b에 대해서 derivative해본다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![rnn link](http://i.imgur.com/z2G6jts.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- U에 대해서 derivative = a\n",
    "- 처음에는 U,W,b가 모두 작아서 J = loss가 크다.\n",
    "- 만약 s = 5, sc = 2 가 나왔으면 J = max(0, 1 - 5 + 2) = 0이 된다. 해당하는 input에 대해서는 loss가 없다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![rnn link](http://i.imgur.com/RtlLYF6.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- W23을 이용하지 않고도 a1을 계산할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![rnn link](http://i.imgur.com/0I5gRIT.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wij로 미분하는 경우 ai, 즉 vector a의 i번째 element만 계산한다.(계산 단축)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![rnn link](http://i.imgur.com/TCXpusf.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- W의 i번째 row와 x의 inner product = sigma를 이용하여 표현\n",
    "- Wij로 미분할 때, k = j가 아닌 Wik들은 모두 상수로서 미분하면 사라진다.\n",
    "- delta i는 Wik와 같은 row 다른 col에 대해서도 사용될 수 있다. 따라서 reuse된다.\n",
    "- f'(z) = f(z)(1-f(z))이므로, 계산이 굉장히 빨라진다. (미분계산이 필요없다. 단순한 computing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![rnn link](http://i.imgur.com/ptf9Zro.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- element i와 j의 조합만큼 계산이 이루어질텐데, 이를 쉽게 하기 위해서 outer product를 사용한다. 계산이 매우 간단해진다! \n",
    "- delta의 dim = [2,1], x transpose의 dim = [1,3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![rnn link](http://i.imgur.com/0xz2uT4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "zi의 derivative로 chain rule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "xi input으로 derivative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![rnn link](http://i.imgur.com/MDjahKp.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- i번째가 아닌 U transpose * a는 ai로 미분했을 때 사라진다. 따라서 Ui만 남는다. \n",
    "- j번째가 아닌 x는 xj로 미분했을 때 사라진다. 따라서 Wij만 남는다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![rnn link](http://i.imgur.com/I78MclP.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "vector x는 xj가 합쳐진 여러 row의 벡터이므로, full gradient는 Wj가 합쳐진 여러 col을 가지는 전체 W가 된다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
