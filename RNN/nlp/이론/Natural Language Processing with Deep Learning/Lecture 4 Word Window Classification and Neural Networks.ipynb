{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 4 | Word Window Classification and Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "강의 출처 : https://www.youtube.com/watch?v=uc2_iwVqrRI&index=4&list=PL3FW7Lu3i5Jsnh1rnUwq_TcylNr7EkRe6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![rnn link](http://i.imgur.com/sAgNpi1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- W의 dim = [C,d]이고, x의 dim = [d,1]이다. (C = # of classes, d = 단어를 vector로 표현하는 길이)\n",
    "- y는 1~C 중에 속해있는 하나의 class이다. (올바른 target) \n",
    "- 각 W의 row(1~C)에 x를 곱한 값들의 합의 분모이다.\n",
    "- 1~C 중에 속해있는 하나의 class y에 해당하는 W의 row(y번째)에 x를 곱한 값이 분자이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![rnn link](http://i.imgur.com/tmstwpR.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 1번 : W의 y번째 row(dim = [1,d])와 x(dim = [d,1])을 내적한 값이다.\n",
    "- 2번 : exp(해당 내적값) / 전체 exp(내적값) = p 확률이 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![rnn link](http://i.imgur.com/99a3UAV.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "objective function = maximizing probability = maximizing log probability = minimizing the negative of that log probability "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![rnn link](http://i.imgur.com/s33gUFL.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- p = 실제 probability distribution [0,0,...,1,...,0,0]\n",
    "- q = softmax 결과\n",
    "- p가 one-hot-vector이므로, p(c) * log q(c)는 결과적으로 log q(c)만 남는다.\n",
    "- 전체 H(p,q) = true class위치의 negative log probability of softmax output\n",
    "- (p(c) = 0 or 1 single number)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![rnn link](http://i.imgur.com/DROlKUB.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위에서는 (x,y)하나의 data에 대해서 loss를 구하였다. 전체 dataset(1...N)에 대한 loss의 평균이 전체 dataset의 loss이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![rnn link](http://i.imgur.com/nWcXUtZ.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "전체 data의 loss를 구하는 과정에서, overfitting을 피하기 위해서 parameter seta를 추가해준다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![rnn link](http://i.imgur.com/aOzA7Aa.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "vector로 나타낸 'telly', 'TV', 'television' 모두 가까운 거리에 위치해있고, classification도 잘 이루어졌다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![rnn link](http://i.imgur.com/IM1K3bG.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 제한된 training set size로 인해 'telly', 'TV' 두 개의 단어만 retrain시켰다.\n",
    "- 두 개의 단어 'telly', 'TV'만 위치가 변경되었고, 이로 인해 boundary가 달라져, 'television'은 misclassify 되었다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![rnn link](http://i.imgur.com/gc5VwSp.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "결론 : small training data set만 있는 경우에는 training 시키지 말아라."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![rnn link](http://i.imgur.com/TSwtTMe.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "단순히 single word만 활용하여 classify 시키는 것은, 위와 같은 단어의 이중적인 의미(모호함) 때문에 좋은 결과를 낼 수 없다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![rnn link](http://i.imgur.com/GHNzO6x.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "따라서 따라서 주변 단어들의 문맥을 보고 해당 단어를 classify하는 것이 window classification이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![rnn link](http://i.imgur.com/kzXgCMe.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- window length = 2 이면, 해당 center word를 중심으로 앞 뒤 2개의 단어를 하나의 vector x로 본다.\n",
    "- 따라서 input으로 사용되는 vector x의 차원도 변한다. 기존의 x의 dim = [d,1] 이였다면, window length = 2인 window vector x의 dim = [5d,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이후에는 softmax함수에 대해 backpropagation의 과정을 거칠 때 사용할만한 tip을 제시해주는데, 설명은 생략하겠다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![rnn link](http://i.imgur.com/WSMbIuZ.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for 문을 사용해서 행렬의 곱셈을 하는 것 보다는, 행렬곱을 이용하는 것이 훨씬 속도가 빠르다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이후에는 logistic regression의 한계를 언급하면서 Neural Net의 도입을 알린다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![rnn link](http://i.imgur.com/uDuqDD8.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- loss를 구하는 다른 방법인 max-margin loss에 대해서 알아본다\n",
    "- S : score of center word = paris = named entity location\n",
    "- Sc : score of center word = museums = not a named entity location\n",
    "- minimize J의 방향으로 training 시켜준다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![rnn link](http://i.imgur.com/z2G6jts.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 처음에는 U,W,b가 모두 작아서 J = loss가 크다.\n",
    "- 만약 s = 5, sc = 2 가 나왔으면 J = max(0, 1 - 5 + 2) = 0이 된다. 해당하는 input에 대해서는 loss가 없다.\n",
    "- J>0인 경우, s와 sc에 대해서 derivatives를 계산한다. (backpropagation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![rnn link](http://i.imgur.com/RtlLYF6.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![rnn link](http://i.imgur.com/0I5gRIT.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![rnn link](http://i.imgur.com/TCXpusf.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Wij로 미분할 때, k = j가 아닌 Wik들은 모두 상수로서 미분하면 사라진다.\n",
    "- f'(z) = f(z)(1-f(z))이므로, 계산이 굉장히 빨라진다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![rnn link](http://i.imgur.com/ptf9Zro.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![rnn link](http://i.imgur.com/0xz2uT4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![rnn link](http://i.imgur.com/MDjahKp.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![rnn link](http://i.imgur.com/I78MclP.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
