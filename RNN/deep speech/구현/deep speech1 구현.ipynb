{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DEEP SPEECH1 구현 - pytorch\n",
    "\n",
    "CTC loss 를 이용한 deep speech 구현이다. <br>\n",
    "구현 모델은 [Deep speech 이론 by YBigTa](https://github.com/YBIGTA/Deep_learning/blob/master/RNN/deep%20speech/%EC%84%A4%EB%AA%85/Deep%20speech_%EC%83%81%ED%97%8C.pdf) 에서 확인 가능하다. <br>\n",
    "\n",
    "본 코드는 [deep speech2 implementation](https://github.com/SeanNaren/deepspeech.pytorch/)을 상당부분 참고한다.\n",
    "\n",
    "### 구현 stack\n",
    "OS : ubuntu 16.04 <br>\n",
    "conda : 4.2.9 <br>\n",
    "\n",
    "\n",
    "## 설치\n",
    "\n",
    "오디오 I/O 를 위한 pytorch audio를 설치한다. <br>\n",
    "```\n",
    "sudo apt-get install sox libsox-dev libsox-fmt-all\n",
    "git clone https://github.com/pytorch/audio.git\n",
    "cd audio\n",
    "python setup.py install\n",
    "```\n",
    "\n",
    "Requirments를 설치한다. <br>\n",
    "```\n",
    "sudo pip install python-levenshtein torch visdom wget librosa\n",
    "```\n",
    "\n",
    "## 데이터셋\n",
    "\n",
    "데이터셋의 경우 카네기 멜론 대학교에서 제공한 free dataset인 **AN4** 를 사용한다.\n",
    "\n",
    "```python\n",
    "import argparse\n",
    "import os\n",
    "import io\n",
    "import shutil\n",
    "import tarfile\n",
    "import wget\n",
    "\n",
    "from utils import create_manifest\n",
    "\n",
    "# command line에서 동작하는 것들을 더 쉽게 만들어주는 parser를 이용하여 데이터를 다운받는다.\n",
    "parser = argparse.ArgumentParser(description='Processes and downloads an4.') \n",
    "parser.add_argument('--target_dir', default='an4_dataset/', help='Path to save dataset')\n",
    "parser.add_argument('--sample_rate', default=16000, type=int, help='Sample rate')\n",
    "args = parser.parse_args()\n",
    "\n",
    "\n",
    "def _format_data(root_path, data_tag, name, wav_folder):\n",
    "    data_path = args.target_dir + data_tag + '/' + name + '/'\n",
    "    new_transcript_path = data_path + '/txt/'\n",
    "    new_wav_path = data_path + '/wav/'\n",
    "\n",
    "    os.makedirs(new_transcript_path)\n",
    "    os.makedirs(new_wav_path)\n",
    "\n",
    "    wav_path = root_path + 'wav/'\n",
    "    file_ids = root_path + 'etc/an4_%s.fileids' % data_tag\n",
    "    transcripts = root_path + 'etc/an4_%s.transcription' % data_tag\n",
    "    train_path = wav_path + wav_folder\n",
    "\n",
    "    _convert_audio_to_wav(train_path)\n",
    "    _format_files(file_ids, new_transcript_path, new_wav_path, transcripts, wav_path)\n",
    "\n",
    "\n",
    "def _convert_audio_to_wav(train_path):\n",
    "    with os.popen('find %s -type f -name \"*.raw\"' % train_path) as pipe:\n",
    "        for line in pipe:\n",
    "            raw_path = line.strip()\n",
    "            new_path = line.replace('.raw', '.wav').strip()\n",
    "            cmd = 'sox -t raw -r %d -b 16 -e signed-integer -B -c 1 \\\"%s\\\" \\\"%s\\\"' % (\n",
    "                args.sample_rate, raw_path, new_path)\n",
    "            os.system(cmd)\n",
    "\n",
    "\n",
    "def _format_files(file_ids, new_transcript_path, new_wav_path, transcripts, wav_path):\n",
    "    with open(file_ids, 'r') as f:\n",
    "        with open(transcripts, 'r') as t:\n",
    "            paths = f.readlines()\n",
    "            transcripts = t.readlines()\n",
    "            for x in range(len(paths)):\n",
    "                path = wav_path + paths[x].strip() + '.wav'\n",
    "                filename = path.split('/')[-1]\n",
    "                extracted_transcript = _process_transcript(transcripts, x)\n",
    "                current_path = os.path.abspath(path)\n",
    "                new_path = new_wav_path + filename\n",
    "                text_path = new_transcript_path + filename.replace('.wav', '.txt')\n",
    "                with io.FileIO(text_path, \"w\") as file:\n",
    "                    file.write(extracted_transcript.encode('utf-8'))\n",
    "                os.rename(current_path, new_path)\n",
    "\n",
    "\n",
    "def _process_transcript(transcripts, x):\n",
    "    extracted_transcript = transcripts[x].split('(')[0].strip(\"<s>\").split('<')[0].strip().upper()\n",
    "    return extracted_transcript\n",
    "\n",
    "\n",
    "def main():\n",
    "    root_path = 'an4/'\n",
    "    name = 'an4'\n",
    "    wget.download('http://www.speech.cs.cmu.edu/databases/an4/an4_raw.bigendian.tar.gz')\n",
    "    tar = tarfile.open('an4_raw.bigendian.tar.gz')\n",
    "    tar.extractall()\n",
    "    os.makedirs(args.target_dir)\n",
    "    _format_data(root_path, 'train', name, 'an4_clstk')\n",
    "    _format_data(root_path, 'test', name, 'an4test_clstk')\n",
    "    shutil.rmtree(root_path)\n",
    "    os.remove('an4_raw.bigendian.tar.gz')\n",
    "    train_path = args.target_dir + '/train/'\n",
    "    test_path = args.target_dir + '/test/'\n",
    "    print ('\\n', 'Creating manifests...')\n",
    "    create_manifest(train_path, 'an4_train')\n",
    "    create_manifest(test_path, 'an4_val')\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Making audio spectogram using fourier transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import scipy.io.wavfile as wavb\n",
    "\n",
    "from numpy.lib import stride_tricks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# audio signal의 fourier transformation\n",
    "# hanning window 에 대한 ref : http://www.incosys.co.kr/index.php/book/1-basic-tech-vibration/2015-01-21-07-36-51-6/2015-01-21-07-36-51-6-1/2015-01-21-07-36-51-6-1-5\n",
    "\"\"\"\n",
    "short time fourier transform of audio signal \n",
    "\"\"\"\n",
    "def stft(sig, frameSize, overlapFac=0.5, window=np.hanning):\n",
    "    win = window(frameSize)\n",
    "    hopSize = int(frameSize - np.floor(overlapFac * frameSize))\n",
    "    \n",
    "    # zeros at beginning (thus center of 1st window should be for sample nr. 0)\n",
    "    samples = np.append(np.zeros(np.floor(frameSize/2.0)), sig)    \n",
    "    # cols for windowing\n",
    "    cols = np.ceil( (len(samples) - frameSize) / float(hopSize)) + 1\n",
    "    # zeros at end (thus samples can be fully covered by frames)\n",
    "    samples = np.append(samples, np.zeros(frameSize))\n",
    "    \n",
    "    frames = stride_tricks.as_strided(samples, shape=(cols, frameSize), strides=(samples.strides[0]*hopSize, samples.strides[0])).copy()\n",
    "    frames *= win\n",
    "    \n",
    "    return np.fft.rfft(frames)    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset과 Dataloader 구현\n",
    "\n",
    "pytorch의 dataset과 dataloader의 경우는 그 클래스가 정해져 있다. <br>\n",
    "\n",
    "따라서 data loader와 dataset의 경우에는 사용자 정의 클래스를 만들기 위해서 **torch.utils.data**의 **Dataset**과 **DataLoader**를 상속받아야 한다. <br>\n",
    "\n",
    "이를 통해 사용자 정의 dataset, dataloader를 구현할 것이며, <br>\n",
    "\n",
    "본 dataset에는 audio spectogram 형태의 dataset이여야 하므로 *wav* 파일을 *spectogram* 형태로 바꾸는 함수를 집어넣고 구현한다., <br>\n",
    "\n",
    "추가적인 dataset, dataloader 구현 튜토리얼은 [pytorch 공인 튜토리얼](http://pytorch.org/tutorials/beginner/data_loading_tutorial.html) 을 참고한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "import scipy.signal\n",
    "import torch\n",
    "import torchaudio\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. modeling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import math\n",
    "from collections import OrderedDict\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\"\"\"\n",
    "RNN 모델상에서의 사용되는 4번째 layer RNN.\n",
    "deep speech(2 아닌 1)논문에서는 bidirectional RNN을 사용한다.\n",
    "\"\"\"\n",
    "supported_rnns = {\n",
    "    'lstm': nn.LSTM,\n",
    "    'rnn': nn.RNN,\n",
    "    'gru': nn.GRU\n",
    "}\n",
    "supported_rnns_inv = dict((v, k) for k, v in supported_rnns.items()) #내가봤을 때 inverse는 딱히 필요없어."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Collapses input of dim T*N*H to (T*N)*H, and applies to a module.\n",
    "Allows handling of variable sequence lengths and minibatch sizes.\n",
    ":param module: Module to apply input to.\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "그니까 이게 minibatch를 하나의 string으로 만들어준다는 거지.\n",
    "Q.  왜 굳이 그렇게하지? \n",
    "그냥 long sentence training을 위하여?\n",
    "\"\"\"\n",
    "class SequenceWise(nn.Module):\n",
    "    def __init__(self, module):\n",
    "        super(SequenceWise, self).__init__()\n",
    "        self.module = module\n",
    "\n",
    "    def forward(self, x):\n",
    "        t, n = x.size(0), x.size(1)\n",
    "        x = x.view(t * n, -1)\n",
    "        x = self.module(x)\n",
    "        x = x.view(t, n, -1)\n",
    "        return x\n",
    "\n",
    "    def __repr__(self):\n",
    "        tmpstr = self.__class__.__name__ + ' (\\n'\n",
    "        tmpstr += self.module.__repr__()\n",
    "        tmpstr += ')'\n",
    "        return tmpstr\n",
    "    \n",
    "\"\"\"\n",
    "이건 굉장히 typical 한 minibatch softmax.\n",
    "각 data의 softmax를 torch.stack을 해야한다는거지.\n",
    "Q. 근데 여기서 self.training은 어디서 오는거야\n",
    "\"\"\"\n",
    "class InferenceBatchSoftmax(nn.Module):\n",
    "    def forward(self, input_):\n",
    "        if not self.training:\n",
    "            batch_size = input_.size()[0]\n",
    "            return torch.stack([F.log_softmax(input_[i]) for i in range(batch_size)], 0)\n",
    "        else:\n",
    "            return input_\n",
    "        \n",
    "\"\"\"\n",
    "batch구현\n",
    "왜 batch normalization data를 sequencewise를 이 단에서 구현하느냐.\n",
    "batch normalization에 대한 ref) https://shuuki4.wordpress.com/2016/01/13/batch-normalization-%EC%84%A4%EB%AA%85-%EB%B0%8F-%EA%B5%AC%ED%98%84/\n",
    "이 sequencewise를 없애고 해보자 나중에.\n",
    "\"\"\"\n",
    "class BatchRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, rnn_type=nn.LSTM, bidirectional=False, batch_norm=True):\n",
    "        super(BatchRNN, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.bidirectional = bidirectional\n",
    "        self.batch_norm = SequenceWise(nn.BatchNorm1d(input_size)) if batch_norm else None\n",
    "        self.rnn = rnn_type(input_size=input_size, hidden_size=hidden_size,\n",
    "                            bidirectional=bidirectional, bias=False)\n",
    "        self.num_directions = 2 if bidirectional else 1\n",
    "\n",
    "    # Q. 그니까 RNN 의 hidden layer를 output으로 내는데, bidirectional의 경우에는 이 둘을 그냥 더한다?\n",
    "    def forward(self, x):\n",
    "        if self.batch_norm is not None:\n",
    "            x = self.batch_norm(x)\n",
    "        x, _ = self.rnn(x)\n",
    "        if self.bidirectional:\n",
    "            x = x.view(x.size(0), x.size(1), 2, -1).sum(2).view(x.size(0), x.size(1), -1)  # (TxNxH*2) -> (TxNxH) by sum\n",
    "            return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "Q. labels의 역할은? 왜 여긴 abc밖에 없는거지?\n",
    "Q. audio_conf는?\n",
    "\"\"\"\n",
    "class DeepSpeech(nn.Module):\n",
    "    def __init__(self, rnn_type=nn.LSTM, labels=\"abc\", rnn_hidden_size=768, nb_layers=5, audio_conf=None,\n",
    "                 bidirectional=True):\n",
    "        super(DeepSpeech, self).__init__()\n",
    "\n",
    "        # model metadata needed for serialization/deserialization\n",
    "        if audio_conf is None:\n",
    "            audio_conf = {}\n",
    "        self._version = '0.0.1'\n",
    "        self._hidden_size = rnn_hidden_size\n",
    "        self._hidden_layers = nb_layers\n",
    "        self._rnn_type = rnn_type\n",
    "        self._audio_conf = audio_conf or {}\n",
    "        self._labels = labels\n",
    "\n",
    "        sample_rate = self._audio_conf.get(\"sample_rate\", 16000)\n",
    "        window_size = self._audio_conf.get(\"window_size\", 0.02)\n",
    "        num_classes = len(self._labels)\n",
    "\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=(41, 11), stride=(2, 2)),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.Hardtanh(0, 20, inplace=True),\n",
    "            nn.Conv2d(32, 32, kernel_size=(21, 11), stride=(2, 1)),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.Hardtanh(0, 20, inplace=True)\n",
    "        )\n",
    "        # Based on above convolutions and spectrogram size using conv formula (W - F + 2P)/ S+1\n",
    "        rnn_input_size = int(math.floor((sample_rate * window_size) / 2) + 1)\n",
    "        rnn_input_size = int(math.floor(rnn_input_size - 41) / 2 + 1)\n",
    "        rnn_input_size = int(math.floor(rnn_input_size - 21) / 2 + 1)\n",
    "        rnn_input_size *= 32\n",
    "\n",
    "        rnns = []\n",
    "        rnn = BatchRNN(input_size=rnn_input_size, hidden_size=rnn_hidden_size, rnn_type=rnn_type,\n",
    "                       bidirectional=bidirectional, batch_norm=False)\n",
    "        rnns.append(('0', rnn))\n",
    "        for x in range(nb_layers - 1):\n",
    "            rnn = BatchRNN(input_size=rnn_hidden_size, hidden_size=rnn_hidden_size, rnn_type=rnn_type,\n",
    "                           bidirectional=bidirectional)\n",
    "            rnns.append(('%d' % (x + 1), rnn))\n",
    "        self.rnns = nn.Sequential(OrderedDict(rnns))\n",
    "        fully_connected = nn.Sequential(\n",
    "            nn.BatchNorm1d(rnn_hidden_size),\n",
    "            nn.Linear(rnn_hidden_size, num_classes, bias=False)\n",
    "        )\n",
    "        self.fc = nn.Sequential(\n",
    "            SequenceWise(fully_connected),\n",
    "        )\n",
    "        self.softmax = InferenceBatchSoftmax()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "\n",
    "        sizes = x.size()\n",
    "        x = x.view(sizes[0], sizes[1] * sizes[2], sizes[3])  # Collapse feature dimension\n",
    "        x = x.transpose(1, 2).transpose(0, 1).contiguous()  # TxNxH\n",
    "\n",
    "        x = self.rnns(x)\n",
    "\n",
    "        x = self.fc(x)\n",
    "        x = x.transpose(0, 1)\n",
    "        x = self.softmax(x)\n",
    "        return x\n",
    "\n",
    "    @classmethod\n",
    "    def load_model(cls, path, cuda=False):\n",
    "        package = torch.load(path, map_location=lambda storage, loc: storage)\n",
    "        model = cls(rnn_hidden_size=package['hidden_size'], nb_layers=package['hidden_layers'],\n",
    "                    labels=package['labels'], audio_conf=package['audio_conf'],\n",
    "                    rnn_type=supported_rnns[package['rnn_type']])\n",
    "        model.load_state_dict(package['state_dict'])\n",
    "        if cuda:\n",
    "            model = torch.nn.DataParallel(model).cuda()\n",
    "        return model\n",
    "\n",
    "    @staticmethod\n",
    "    def serialize(model, optimizer=None, epoch=None, iteration=None, loss_results=None,\n",
    "                  cer_results=None, wer_results=None, avg_loss=None, meta=None):\n",
    "        model_is_cuda = next(model.parameters()).is_cuda\n",
    "        model = model.module if model_is_cuda else model\n",
    "        package = {\n",
    "            'version': model._version,\n",
    "            'hidden_size': model._hidden_size,\n",
    "            'hidden_layers': model._hidden_layers,\n",
    "            'rnn_type': supported_rnns_inv.get(model._rnn_type, model._rnn_type.__name__.lower()),\n",
    "            'audio_conf': model._audio_conf,\n",
    "            'labels': model._labels,\n",
    "            'state_dict': model.state_dict()\n",
    "        }\n",
    "        if optimizer is not None:\n",
    "            package['optim_dict'] = optimizer.state_dict()\n",
    "        if avg_loss is not None:\n",
    "            package['avg_loss'] = avg_loss\n",
    "        if epoch is not None:\n",
    "            package['epoch'] = epoch + 1  # increment for readability\n",
    "        if iteration is not None:\n",
    "            package['iteration'] = iteration\n",
    "        if loss_results is not None:\n",
    "            package['loss_results'] = loss_results\n",
    "            package['cer_results'] = cer_results\n",
    "            package['wer_results'] = wer_results\n",
    "        if meta is not None:\n",
    "            package['meta'] = meta\n",
    "        return package\n",
    "\n",
    "    @staticmethod\n",
    "    def get_labels(model):\n",
    "        model_is_cuda = next(model.parameters()).is_cuda\n",
    "        return model.module._labels if model_is_cuda else model._labels\n",
    "\n",
    "    @staticmethod\n",
    "    def get_param_size(model):\n",
    "        params = 0\n",
    "        for p in model.parameters():\n",
    "            tmp = 1\n",
    "            for x in p.size():\n",
    "                tmp *= x\n",
    "            params += tmp\n",
    "        return params\n",
    "\n",
    "    @staticmethod\n",
    "    def get_audio_conf(model):\n",
    "        model_is_cuda = next(model.parameters()).is_cuda\n",
    "        return model.module._audio_conf if model_is_cuda else model._audio_conf\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(\"abc\")"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:torch]",
   "language": "python",
   "name": "conda-env-torch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
